{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Solving Process\n",
    "================\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Understand the problem and the data\n",
    "#### 1. Load Data\n",
    "#### 2. Checking Datas\n",
    "[2.1 Basic Data Checking](#2.1-Basic-Data-Checking)  \n",
    "[2.2 Checking Missing Data](#2.2-Checking-Missing-Data)   \n",
    "[2.3 Checking Target Label](#2.3-Checking-Target-Label)    \n",
    "[2.4 Checking Data Types](#2.4-Checking-Data-Types)  \n",
    "[2.5 Checking for Outliers](#2.5-Checking-for-Outliers)  \n",
    "#### 3. Explanatory Data Analysis\n",
    "[3.1 Checking relation between all the featurea and target](#3.1-Checking-relation-between-all-the-featurea-and-target)   \n",
    "[3.2 Visualize the relation between all the features and target](#3.2-Visualize-the-relation-between-all-the-features-and-target)  \n",
    "[3.3 Visualize relationship between 2 features](#3.3-Visualize-relationship-between-2-features)  \n",
    "[3.4 특성 합쳐서 분석해보기](#3.4-특성-합쳐서-분석해보기)  \n",
    "[3.5 비대칭 정보에 대해 log 취해주기](#3.5-비대칭-정보에-대해-log-취해주기)  \n",
    "[3.6 Correlation 확인해보기](#3.6-Correlation-확인해보기)\n",
    "#### 4. Feature Engineering\n",
    "[4.1 Fill Null values](#4.1-Fill-Null-values)  \n",
    "[4.2 구간 데이터를 범주 데이터로 바꿔보기](#4.2-구간-데이터를-범주-데이터로-바꿔보기)  \n",
    "[4.3 문자열 데이터를 수치형 데이터로 바꾸기](#4.3-문자열-데이터를-수치형-데이터로-바꾸기)    \n",
    "[4.4 Heatmap으로 시각화해보기](#4.4-Heatmap으로-시각화해보기)  \n",
    "[4.5 Polynominal Features](#4.5-Polynominal-Features)  \n",
    "[4.6 Domain Knowledge Features](#4.6-Domain-Knowledge-Features)  \n",
    "[4.5 One-Hot Encoding](#4.5-One-Hot-Encoding)  \n",
    "[4.6 Aligning Training and Testing Data](#4.6-Aligning-Training-and-Testing-Data)  \n",
    "[4.6 Drop Columns](#4.6-Drop-Columns)  \n",
    "[4.7 Feature Scaling](#4.7-Feature-Scaling)  \n",
    "#### 5. Model Selection\n",
    "[5.1 train, validation, test set 분리해주기](#5.1-train,-validation,-test-set-분리해주기)  \n",
    "[5.2 Model Lookthrough](#5.2-Model-Lookthrough)  \n",
    "[5.3 Hyperparameter Tuning](#5.3-Hyperparameter-Tuning)  \n",
    "[5.4 Plot Learning Curve](#5.4-Plot-Learning-Curve)  \n",
    "[5.5 Test on Engineered Features](#5.5-Test-on-Engineered-Features)  \n",
    "#### 6. Evaluation and Application\n",
    "[6.1 Evaluate Accuracy](#6.1-Evaluate-Accuracy)  \n",
    "[6.2 Feature importace](#6.2-Feature-importace)  \n",
    "[6.3 Submission](#6.3-Submission)  \n",
    "\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Understand the Problem and the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0.1 데이터 파일과 컬럼 내용 파악하기 \n",
    "- 컬럼 설명 확인하기\n",
    "#### 0.2 평가 지표 확인하기\n",
    "- 대회에서 어떤 평가지표(roc-auc,f1 등)를 사용하지는 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking Datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Basic Data Checking  \n",
    "- 데이터 파일과 컬럼 내용 출력해서 확인해보기  \n",
    "- train, test set 각각 읽어들이고 shape, head 확인해보기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n",
    "train.tail()\n",
    "train.shape\n",
    "train.info()\n",
    "train.describe()\n",
    "\n",
    "test.head()\n",
    "test.tail()\n",
    "test.shape\n",
    "test.info()\n",
    "test.describe()\n",
    "\n",
    "rows = train.shape[0]\n",
    "columns = train.shape[1]\n",
    "print('The train dataset contains {} rows and {} columns'.format(rows, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Checking Missing Data  \n",
    "- train, test 데이터 모두 확인해보기\n",
    "- msno로 시각화 해보기 -> (누락된 데이터가 너무 많으면 사용하지 않음!)\n",
    "- missing value 확인해보기\n",
    "- 나중에 Imputer를 사용해 missing values를 채워야 한다.\n",
    "      하지만 XGBoost는 imputer 방식 없이도 missing value를 처리할 수 있다. \n",
    "- 너무 많은 missing value를 가지고 있는 column은 drop해줄 수 있다. \n",
    "- 어떤 column이 유용한지 모르므로 일단 가지고 있는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values 비율 확인하는 방법 1(column의 수가 적을 때!)\n",
    "for col in df_train.columns:\n",
    "    msg = 'column : {:>10}\\t Percent of Nan value : {:.2f}%'.format(col, 100 * (df_train[col].isnull().sum() / df_train[col].shape[0]))\n",
    "    print(msg)\n",
    "    \n",
    "for col in df_test.columns:\n",
    "    msg = 'column : {:>10}\\t Percent of Nan value : {:.2f}%'.format(col, 100 * (df_test[col].isnull().sum() / df_test[col].shape[0]))\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values 비율 확인하는 방법 2(column의 수가 많을 때!)\n",
    "def missing_values_table(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', \n",
    "                   1 : '% of Total Values'}\n",
    "    )\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "        mis_val_table_ren_columns.iloc[:, 1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "    \n",
    "    print('Your selected dataframe has ' + str(df.shape[1]) + 'columns.\\n'\n",
    "         'There are ' + str(mis_val_table_ren_columns.shape[0]) +\n",
    "         ' columns that have missing values.')\n",
    "    \n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing value 시각화\n",
    "msno.matrix(df=df_train.iloc[:, :], figsize=(8,8), color=(0.8, 0.5, 0.2))\n",
    "\n",
    "msno.matrix(df=df_train.iloc[:, :], figsize=(8,8), color=(0.8, 0.5, 0.2))\n",
    "\n",
    "msno.bar(df=df_test.iloc[:, :], figsize=(8, 8), color=(0.8, 0.5, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Checking Target Label  \n",
    "- 0,1로 분류 가능한 레이블이 어떤 분포를 가지고 있는지 시각적으로 확인해보기  \n",
    "- 극단적인 분포면 도움이 되지 않음..\n",
    "- 수로 나타내보고, 시각화해봐서 확인해보기\n",
    "- unbalanced 문제 해결 방법 : http://www.chioka.in/class-imbalance-problem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화해보기\n",
    "fig, ax = plt.subplots(1,2, figsize = (18,8))\n",
    "\n",
    "df_train['Survived'].value_counts().plot.pie(explode = [0, 0.1], \n",
    "                                             autopct='%1.1f%%',\n",
    "                                            ax=ax[0],\n",
    "                                            shadow=True)\n",
    "ax[0].set_title('Pie plot - Survived')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('Survived', data=df_train, ax=ax[1])\n",
    "ax[1].set_title('Count plot - Survived')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수로 나타내보기\n",
    "app_train['TARGET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Checking Data Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- int64, float64(수치형), object(string or categorical)\n",
    "- 데이터 타입별 컬럼 수 확인\n",
    "- object column 각각에 별개의 값이 몇 개 있는지 확인\n",
    "- MetaData 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입별 column의 수 세보기\n",
    "app_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MetaData 만들기\n",
    "data = []\n",
    "for f in train.columns:\n",
    "    # Defining the role\n",
    "    if f == 'target':\n",
    "        role = 'target'\n",
    "    elif f == 'id':\n",
    "        role = 'id'\n",
    "    else:\n",
    "        role = 'input'\n",
    "         \n",
    "    # Defining the level\n",
    "    if 'bin' in f or f == 'target':\n",
    "        level = 'binary'\n",
    "    elif 'cat' in f or f == 'id':\n",
    "        level = 'nominal'\n",
    "    elif train[f].dtype == float:\n",
    "        level = 'interval'\n",
    "    elif train[f].dtype == int:\n",
    "        level = 'ordinal'\n",
    "        \n",
    "    # Initialize keep to True for all variables except for id\n",
    "    keep = True\n",
    "    if f == 'id':\n",
    "        keep = False\n",
    "    \n",
    "    # Defining the data type \n",
    "    dtype = train[f].dtype\n",
    "    \n",
    "    # Creating a Dict that contains all the metadata for the variable\n",
    "    f_dict = {\n",
    "        'varname': f,\n",
    "        'role': role,\n",
    "        'level': level,\n",
    "        'keep': keep,\n",
    "        'dtype': dtype\n",
    "    }\n",
    "    data.append(f_dict)\n",
    "    \n",
    "meta = pd.DataFrame(data, columns=['varname', 'role', 'level', 'keep', 'dtype'])\n",
    "meta.set_index('varname', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Checking for Outliers\n",
    "- 이상치 처리는 상황에 따르나 일반적으로 missing value로 처리한 후 머신러닝을 적용하기 전에 Imputation을 진행한다. \n",
    "- 하지만 현재 DAYS_EMPLOYED column은 중요한 정보를 담고 있으므로 다른 값으로 채워주기로 함\n",
    "- test set에도 마찬가지로 적용  \n",
    "- 이상치 데이터가 target에 어떤 영향을 주는 지 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column마다 이상치 여부 확인해보기\n",
    "app_train['DAYS_EMPLOYED'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection \n",
    "\n",
    "def detect_outliers(df,n,features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "    \n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col],75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "        \n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n",
    "        \n",
    "        # append the found outlier indices for col to the list of outlier indices \n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "        \n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)        \n",
    "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n",
    "    \n",
    "    return multiple_outliers   \n",
    "\n",
    "# detect outliers from Age, SibSp , Parch and Fare\n",
    "Outliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explanatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Checking relation between all the featurea and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index = True).count()\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).sum()\n",
    "pd.crosstab(df_train['Pclass'], df_train['Survived'], margins=True).style.background_gradient(cmap='summer_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualize the relation between all the features and target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***bar, pie, distplot, countplot, violinplot, kdeplot 등을 활용***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countplot\n",
    "y_position = 1.02\n",
    "fig, ax = plt.subplots(1,2,figsize=(18,8))\n",
    "df_train['Pclass'].value_counts().plot.bar(color=['#CD7F32','#FFDF00','#D3D3D3'], ax=ax[0])\n",
    "ax[0].set_title('Number of Passengers By Pclass', y=y_position)\n",
    "ax[0].set_ylabel('Count')\n",
    "sns.countplot('Pclass', hue='Survived', data=df_train, ax=ax[1])\n",
    "ax[1].set_title('Pclass : Survived vs Dead', y=y_position)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kdeplot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 5))\n",
    "sns.kdeplot(df_train.dropna()[df_train.dropna()['Survived'] == 1]['Age'], ax=ax)\n",
    "sns.kdeplot(df_train.dropna()[df_train.dropna()['Survived'] == 0]['Age'], ax=ax)\n",
    "plt.legend(['Survived == 1', 'Survived == 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violinplot\n",
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=df_train, scale='count', split=True,ax=ax[0])\n",
    "ax[0].set_title('Pclass and Age vs Survived')\n",
    "ax[0].set_yticks(range(0,110,10))\n",
    "sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=df_train, scale='count', split=True,ax=ax[1])\n",
    "ax[1].set_title('Sex and Age vs Survived')\n",
    "ax[1].set_yticks(range(0,110,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Visualize relationship between 2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorplot 1\n",
    "sns.factorplot('Pclass', 'Survived', hue = 'Sex', data=df_train, size=6, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# factorplot 2\n",
    "sns.factorplot(x='Sex', y='Survived', col='Pclass', data=df_train, saturation=.5,\n",
    "              size=9, aspect=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 특성 합쳐서 분석해보기  \n",
    "#### (예 형제,자매 + 부모,자녀 = 가족)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['FamilySize'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "df_test['FamilySize'] = df_test['SibSp'] + df_test['Parch'] + 1\n",
    "\n",
    "print('Maximum size of Family :', df_train['FamilySize'].max())\n",
    "print('Minimum size of Family :', df_train['FamilySize'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 비대칭 정보에 대해 log 취해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distplot\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "g = sns.distplot(df_train['Fare'], color='b', label='Skewness : {:.2f}'.format(df_train['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')\n",
    "\n",
    "df_test.loc[df_test.Fare.isnull(), 'Fare'] = df_test['Fare'].mean()\n",
    "\n",
    "df_train['Fare'] = df_train['Fare'].map(lambda i : np.log(i) if i > 0 else 0)\n",
    "df_test['Fare'] = df_test['Fare'].map(lambda i : np.log(i) if i >0 else 0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(8,8))\n",
    "g = sns.distplot(df_train['Fare'], color = 'b', label = 'Skewness : {:.2f}'.format(df_train['Fare'].skew()), ax=ax)\n",
    "g = g.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Correlation 확인해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- variable과 target 사이의 상관관계 파악\n",
    "- 참고 \n",
    ".00-.19 “very weak”  \n",
    ".20-.39 “weak”  \n",
    ".40-.59 “moderate”  \n",
    ".60-.79 “strong”  \n",
    ".80-1.0 “very strong”  \n",
    "- 고객의 나이가 가장 큰 양의 상관관계를 가짐(하지만 나이는 음수 값이므로 절대값 수치 확인이 필요하다)\n",
    "- 중요한 column은 시각화 해보기\n",
    "- kde plot이 target에 미치는 column의 영향을 확인하기에 좀 더 용이하다. \n",
    "- 나이의 경우 구간으로 나눠 상환률을 확인하는 방법이 있다. \n",
    "- Pairplot은 여러 변수들 간의 관계를 확인하기 유용하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상관관계 높은 순으로 확인해보기\n",
    "correlations = app_train.corr()['TARGET'].sort_values()\n",
    "\n",
    "print('Most Positive Correlations : \\n', correlations.tail(15))\n",
    "print('\\nMost Negative Correlations : \\n', correlations.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화로 상관관계 파악해보기 \n",
    "# EXT_SOURCE_3이 target과 가장 큰 차이를 보인다. \n",
    "# 약한 상관관계지만 머신러닝 모델에 유용한 정보를 제공한다. \n",
    "plt.figure(figsize = (10, 12))\n",
    "\n",
    "for i, source in enumerate(['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']):\n",
    "    \n",
    "    plt.subplot(3, 1, i + 1)\n",
    "    # plot repaid loans\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 0, source].dropna(), label = 'target == 0')\n",
    "    # plot loans that were not repaid\n",
    "    sns.kdeplot(app_train.loc[app_train['TARGET'] == 1, source].dropna(), label = 'target == 1')\n",
    "    \n",
    "    # Label the plots\n",
    "    plt.title('Distribution of %s by Target Value' % source)\n",
    "    plt.xlabel('%s' % source); plt.ylabel('Density');\n",
    "    \n",
    "plt.tight_layout(h_pad = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Feature Engineering\n",
    "- Feature engineering에는 새로운 feature를 만들어내거나 feature를 select하는 과정이 포함되어 있다.\n",
    "- Good column to read : https://blog.featurelabs.com/secret-to-data-science-success/\n",
    "- automated tools : https://docs.featuretools.com/getting_started/install.html\n",
    "- Polynominal Features : 현재 있는 feauture의 제곱 값과 여러 개별 variable의 조합인 interaction term을 생성한다.  \n",
    "  개별 특성은 큰 영향이 없을지라도 특성의 조합으로 target에 큰 영향을 줄 수 있다.   \n",
    "  너무 크게 degree를 설정하면 안된다(overfitting문제 발생)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Fill Null values  \n",
    "- 다른 특성을 이용하여 누락 데이터 채우기  \n",
    "( 예 이름에서 이니셜을 뽑아내고 이니셜의 평균 나이로 누락된 나이 데이터 채우기)  \n",
    "- 가장 많은 소속으로 채우기  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***다른 특성을 이용하여 Null value 채우기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial 뽑아내기\n",
    "df_train['Initial'] = df_train.Name.str.extract('([A-Za-z]+)\\.')\n",
    "df_test['Initial'] = df_test.Name.str.extract('([A-Za-z]+)\\.')\n",
    "\n",
    "# crosstab으로 수 확인\n",
    "pd.crosstab(df_train['Initial'], df_train['Sex']).T.style.background_gradient(cmap='summer_r')\n",
    "\n",
    "# 각 initial별 평균 나이 확인\n",
    "df_train['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n",
    "\n",
    "df_test['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don', 'Dona'],\n",
    "                        ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr', 'Mr'],inplace=True)\n",
    "df_train.groupby('Initial').mean()\n",
    "\n",
    "# initial별 평균으로 null 값 채워주기\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mr'),'Age'] = 33\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Mrs'),'Age'] = 36\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Master'),'Age'] = 5\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Miss'),'Age'] = 22\n",
    "df_train.loc[(df_train.Age.isnull())&(df_train.Initial=='Other'),'Age'] = 46\n",
    "\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mr'),'Age'] = 33\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Mrs'),'Age'] = 36\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Master'),'Age'] = 5\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Miss'),'Age'] = 22\n",
    "df_test.loc[(df_test.Age.isnull())&(df_test.Initial=='Other'),'Age'] = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Imputer\n",
    "# fit on the training data\n",
    "imputer.fit(train)\n",
    "\n",
    "# transform both training and testing data\n",
    "train = imputer.transform(train)\n",
    "test = imputer.transform(app_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***가장 많은 소속으로 null value 채우기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Embarked has ', sum(df_train['Embarked'].isnull()), 'Null values')\n",
    "df_train['Embarked'].fillna('S', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 구간 데이터를 범주 데이터로 바꿔보기  \n",
    "- 직접 다 바꾸거나  \n",
    "- 함수를 활용하거나(apply)  \n",
    "- pandas qcut으로 전달된 bins에 따라 구간 나눠줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method1\n",
    "df_train['Age_cat'] = 0\n",
    "df_train.loc[df_train['Age'] < 10, 'Age_cat'] = 0\n",
    "df_train.loc[(10 <= df_train['Age']) & (df_train['Age'] < 20), 'Age_cat'] = 1\n",
    "df_train.loc[(20 <= df_train['Age']) & (df_train['Age'] < 30), 'Age_cat'] = 2\n",
    "df_train.loc[(30 <= df_train['Age']) & (df_train['Age'] < 40), 'Age_cat'] = 3\n",
    "df_train.loc[(40 <= df_train['Age']) & (df_train['Age'] < 50), 'Age_cat'] = 4\n",
    "df_train.loc[(50 <= df_train['Age']) & (df_train['Age'] < 60), 'Age_cat'] = 5\n",
    "df_train.loc[(60 <= df_train['Age']) & (df_train['Age'] < 70), 'Age_cat'] = 6\n",
    "df_train.loc[70 <= df_train['Age'], 'Age_cat'] = 7\n",
    "\n",
    "df_test['Age_cat'] = 0\n",
    "df_test.loc[df_test['Age'] < 10, 'Age_cat'] = 0\n",
    "df_test.loc[(10 <= df_test['Age']) & (df_test['Age'] < 20), 'Age_cat'] = 1\n",
    "df_test.loc[(20 <= df_test['Age']) & (df_test['Age'] < 30), 'Age_cat'] = 2\n",
    "df_test.loc[(30 <= df_test['Age']) & (df_test['Age'] < 40), 'Age_cat'] = 3\n",
    "df_test.loc[(40 <= df_test['Age']) & (df_test['Age'] < 50), 'Age_cat'] = 4\n",
    "df_test.loc[(50 <= df_test['Age']) & (df_test['Age'] < 60), 'Age_cat'] = 5\n",
    "df_test.loc[(60 <= df_test['Age']) & (df_test['Age'] < 70), 'Age_cat'] = 6\n",
    "df_test.loc[70 <= df_test['Age'], 'Age_cat'] = 7\n",
    "\n",
    "# method 2\n",
    "def category_age(x):\n",
    "    if x < 10:\n",
    "        return 0\n",
    "    elif x < 20:\n",
    "        return 1\n",
    "    elif x < 30:\n",
    "        return 2\n",
    "    elif x < 40:\n",
    "        return 3\n",
    "    elif x < 50:\n",
    "        return 4\n",
    "    elif x < 60:\n",
    "        return 5\n",
    "    elif x < 70:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "    \n",
    "df_train['Age_cat_2'] = df_train['Age'].apply(category_age)\n",
    "\n",
    "df_train.drop(['Age', 'Age_cat_2'], axis=1, inplace=True)\n",
    "df_test.drop(['Age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 문자열 데이터를 수치형 데이터로 바꾸기  \n",
    "- map 활용(Series용)  \n",
    "- apply는 DataFrame용!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Initial'] = df_train['Initial'].map({'Master' : 0, 'Miss' : 1, 'Mr' : 2, 'Mrs' : 3, 'Other' : 4})\n",
    "df_test['Initial'] = df_test['Initial'].map({'Master': 0, 'Miss': 1, 'Mr': 2, 'Mrs': 3, 'Other': 4})\n",
    "\n",
    "df_train['Embarked'].unique()\n",
    "\n",
    "df_train['Embarked'].value_counts()\n",
    "\n",
    "df_train['Embarked'] = df_train['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "df_test['Embarked'] = df_test['Embarked'].map({'C': 0, 'Q': 1, 'S': 2})\n",
    "\n",
    "df_train['Embarked'].isnull().any()\n",
    "\n",
    "df_train['Sex'] = df_train['Sex'].map({'female': 0, 'male': 1})\n",
    "df_test['Sex'] = df_test['Sex'].map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Heatmap으로 시각화해보기  \n",
    "- 예측하고자 하는 레이블과 가장 강한 상관관계를 가지고 있는 특성 파악하기  \n",
    "- 서로 상관관계가 강한 특성들이 있는지 확인해보기  \n",
    "(상관관계가 1 혹은 -1이 나온다는 것은 얻을 수 있는 정보가 하나라는 뜻!)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = df_train[['Survived', 'Pclass', 'Sex', 'Fare', 'Embarked', 'FamilySize', 'Initial', 'Age_cat']]\n",
    "\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "sns.heatmap(heatmap_data.astype(float).corr(), linewidth=0.1, vmax=1.0,\n",
    "           square = True, cmap=colormap, linecolor='white', annot=True, annot_kws={'size' : 16})\n",
    "\n",
    "del heatmap_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Polynominal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- feautre 추출 후 imputer 적용, target label 제거, create polynominal,  \n",
    "  train, transform, 상관관계 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynominal을 적용할 feature들 추출\n",
    "poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\n",
    "poly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n",
    "\n",
    "# imputer for handling missing values\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "# target value 제거해주기\n",
    "poly_target = poly_features['TARGET']\n",
    "poly_features = poly_features.drop(columns = ['TARGET'])\n",
    "\n",
    "# Need to impute missing values\n",
    "# note 3.1 참고\n",
    "poly_features = imputer.fit_transform(poly_features)\n",
    "poly_features_test = imputer.transform(poly_features_test)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "                                  \n",
    "# Create the polynomial object with specified degree\n",
    "poly_transformer = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Train the polynomial features\n",
    "poly_transformer.fit(poly_features)\n",
    "\n",
    "# Transform the features\n",
    "poly_features = poly_transformer.transform(poly_features)\n",
    "poly_features_test = poly_transformer.transform(poly_features_test)\n",
    "print('Polynomial Features shape: ', poly_features.shape)\n",
    "\n",
    "# 새롭게 생성된 feature 이름 확인해보기\n",
    "poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])[:15]\n",
    "\n",
    "# 몇몇 feature들은 target과 오리지널보다 더 큰 상관관계를 보임\n",
    "# Create a dataframe of the features \n",
    "poly_features = pd.DataFrame(poly_features, \n",
    "                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Add in the target\n",
    "poly_features['TARGET'] = poly_target\n",
    "\n",
    "# Find the correlations with the target\n",
    "poly_corrs = poly_features.corr()['TARGET'].sort_values()\n",
    "\n",
    "# Display most negative and most positive\n",
    "print(poly_corrs.head(10), '\\n')\n",
    "print(poly_corrs.tail(5))\n",
    "\n",
    "# train, test set 에 합쳐주기!\n",
    "# Put test features into dataframe\n",
    "poly_features_test = pd.DataFrame(poly_features_test, \n",
    "                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n",
    "                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n",
    "\n",
    "# Merge polynomial features into training dataframe\n",
    "poly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\n",
    "app_train_poly = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Merge polnomial features into testing dataframe\n",
    "poly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\n",
    "app_test_poly = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "# Align the dataframes\n",
    "app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)\n",
    "\n",
    "# Print out the new shapes\n",
    "print('Training data with polynomial features shape: ', app_train_poly.shape)\n",
    "print('Testing data with polynomial features shape:  ', app_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Domain Knowledge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT_INCOME_PERCENT, ANNUITY_INCOME_PERCENT, CREDIT_TERM, DAYS_EMPLOYED_PERCENT 특성 만들어내기\n",
    "app_train_domain = app_train.copy()\n",
    "app_test_domain = app_test.copy()\n",
    "\n",
    "app_train_domain['CREDIT_INCOME_PERCENT'] = app_train_domain['AMT_CREDIT'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['ANNUITY_INCOME_PERCENT'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_INCOME_TOTAL']\n",
    "app_train_domain['CREDIT_TERM'] = app_train_domain['AMT_ANNUITY'] / app_train_domain['AMT_CREDIT']\n",
    "app_train_domain['DAYS_EMPLOYED_PERCENT'] = app_train_domain['DAYS_EMPLOYED'] / app_train_domain['DAYS_BIRTH']\n",
    "\n",
    "app_test_domain['CREDIT_INCOME_PERCENT'] = app_test_domain['AMT_CREDIT'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['ANNUITY_INCOME_PERCENT'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_INCOME_TOTAL']\n",
    "app_test_domain['CREDIT_TERM'] = app_test_domain['AMT_ANNUITY'] / app_test_domain['AMT_CREDIT']\n",
    "app_test_domain['DAYS_EMPLOYED_PERCENT'] = app_test_domain['DAYS_EMPLOYED'] / app_test_domain['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 One-Hot Encoding  \n",
    "- one-hot 인코딩으로 범주형 데이터 변형시켜주기(pd.get_dummies)   \n",
    "- sklearn Labelencoder + OnehotEncoder로도 가능!)  \n",
    "- Label Encoding : 레이블이 임의로 배정됨, categorical value가 단 두 개(남/여)일 뿐일 때는 잘 먹히나 그렇지 않을 시에는...\n",
    "- One-Hot Encoding : categorical value를 잘 표현함, 레이블 수가 크게 증가하는 문제가 있으나 PCA로 해결할 수 있음\n",
    "- 여기서는 2개의 category에는 Label Encoding을, 2개 이상의 category에는 One-Hot Encoding을 사용하기로 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            le.fit(app_train[col])\n",
    "            \n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            \n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "df_train = pd.get_dummies(df_train, columns=['Initial'], prefix='Initial')\n",
    "df_test = pd.get_dummies(df_test, columns=['Initial'], prefix='Initial')\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "df_train = pd.get_dummies(df_train, columns=['Embarked'], prefix='Embarked')\n",
    "df_test = pd.get_dummies(df_test, columns=['Embarked'], prefix='Embarked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHE 후 shape 확인해주기\n",
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)\n",
    "\n",
    "print('Training Features shape :', app_train.shape)\n",
    "print('Testing Features shape :', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Aligning Training and Testing Data\n",
    "- train set과 test set에 같은 수의 feature가 있어야 한다. 하지만 One-Hot Encoding으로 인해 두 수가 같지 않다. 따라서 align을 진행해줘야 한다.(axis=1로 설정해 column에 적용!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align training and testing data\n",
    "train_labels = app_train['TARGET']\n",
    "\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis=1)\n",
    "\n",
    "app_train[\"TARGET\"] = train_labels\n",
    "\n",
    "print('Training Features shape :', app_train.shape)\n",
    "print('Testing Features shape :', app_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Drop Columns  \n",
    "- 불필요한 column들 drop해주기  \n",
    "- 예측하고자하느 레이블 제외하고 column이 같은지 확인해주기  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "df_test.drop(['PassengerId', 'Name',  'SibSp', 'Parch', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "\n",
    "df_train.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0, 1))\n",
    "scaler.fit(train)\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "print('Training data shape :', train.shape)\n",
    "print('Testing data shape :', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 train, validation, test set 분리해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1.1 \n",
    "X_train = df_train.drop('Survived', axis=1).values\n",
    "target_label = df_train['Survived'].values\n",
    "X_test = df_test.values\n",
    "\n",
    "X_tr, X_vld, y_tr, y_vld = train_test_split(X_train, target_label, test_size=0.3, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Lookthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())\n",
    "\n",
    "cv_results = []\n",
    "for classifier in classifiers:\n",
    "    cv_results.append(cross_val_score(classifier, X_train, y = target_label, scoring='accuracy', cv = kfold, n_jobs=4))\n",
    "    \n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "    \n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\n",
    "                       \"CrossValerrors\": cv_std,\n",
    "                       \"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "                                    \"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\n",
    "                                    \"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\n",
    "                                    \"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train,target_label)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_\n",
    "\n",
    "gsadaDTC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Plot Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "g = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,target_label,cv=kfold)\n",
    "g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,target_label,cv=kfold)\n",
    "g = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,target_label,cv=kfold)\n",
    "g = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,target_label,cv=kfold)\n",
    "g = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,target_label,cv=kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Test on Engineered Features\n",
    "- poly feature에 대해 impute, scaling 진행하고 train하기   \n",
    "  -> 결과 보니까 별로 도움안 됨...\n",
    "- domain feature에 대한 검사  \n",
    "  -> 별로 도움 안되보이지만 boosting 모델에선 좋은 결과 보임 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features_names = list(app_train_poly.columns)\n",
    "\n",
    "# impute\n",
    "imputer = Imputer(strategy = 'median')\n",
    "\n",
    "poly_features = imputer.fit_transform(app_train_poly)\n",
    "poly_features_test = imputer.transform(app_test_poly)\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "poly_features = scaler.fit_transform(poly_features)\n",
    "poly_feature_test = scaler.transform(poly_features_test)\n",
    "\n",
    "random_forest_poly = RandomForestClassifier(n_estimators=100,\n",
    "                                           random_state=50,\n",
    "                                           verbose=1,\n",
    "                                           n_jobs=-1)\n",
    "\n",
    "# train\n",
    "random_forest_poly.fit(poly_features, train_labels)\n",
    "\n",
    "# predict\n",
    "predictions = random_forest_poly.predict_proba(poly_features_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation and Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_tr, y_tr)\n",
    "prediction = model.predict(X_vld)\n",
    "\n",
    "print('총 {}명 중 {:.2f}% 정확도로 생존을 맞춤'.format(y_vld.shape[0], 100 * metrics.accuracy_score(prediction, y_vld)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature importace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(df):\n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Show the feature importances for the default features\n",
    "feature_importances_sorted = plot_feature_importances(feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/gender_submission.csv')\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "submission['Survived'] = prediction\n",
    "\n",
    "submission.to_csv('data/my_first_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
