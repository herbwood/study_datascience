{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10.train_techniques.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II_NcSilVwM5",
        "colab_type": "text"
      },
      "source": [
        "Weight Initialization\n",
        "======\n",
        "\n",
        "## 1. Problem occuring during training Neural Network\n",
        "\n",
        "####  Vanishing Gradient \n",
        "MLP 를 학습시키는 방법인 Backpropagation 중 Gradient 항이 사라지는 문제 입니다. 이 항이 0이나 0에 가까워져 학습이 불가능해지는 현상\n",
        "\n",
        "## 2. How to solve Vanishing Gradient?\n",
        "\n",
        "#### 1) Activation Function\n",
        "Vanishing Gradient 현상은 Activation Function 의 변경을 통해 어느정도 보완할 수 있게 되었습니다. \n",
        "\n",
        "### 2) Weight Initialization\n",
        "***아니면 처음부터 weight 값들이 최적의 값들이라면 gradient 가 작아지더라도 훌륭한 모델이 생성될 수 있겠지요.***  \n",
        "***또한 처음부터 weight 값들이 최적의 값들이라면 학습횟수가 많지 않아도 훌륭한 모델을 만들 수 있을 것입니다.***\n",
        "\n",
        "## 3. Various types of Activation functions and Weight Initialization Methods\n",
        "\n",
        "Weight Initialization | Activation functions\n",
        "----------------------|--------------------\n",
        "Xavier(Glorot)|tanh, sigmoid, softmax\n",
        "He| ReLU, Leacky ReLU, PReLU, ELU\n",
        "LeCun | SELU\n",
        "\n",
        "### Reference : https://gomguard.tistory.com/183\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vclJIQzE0KBy",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "from keras.datasets import reuters\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.layers.noise import AlphaDropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "max_words = 1000\n",
        "batch_size = 16\n",
        "epochs = 40\n",
        "plot = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qeg0JIRB0KlY",
        "colab": {}
      },
      "source": [
        "def create_network(n_dense=6,\n",
        "                   dense_units=16,\n",
        "                   activation='selu',\n",
        "                   dropout=AlphaDropout,\n",
        "                   dropout_rate=0.1,\n",
        "                   kernel_initializer='lecun_normal',\n",
        "                   optimizer='adam',\n",
        "                   num_classes=1,\n",
        "                   max_words=max_words):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(dense_units, input_shape=(max_words,),\n",
        "                    kernel_initializer=kernel_initializer))\n",
        "    model.add(Activation(activation))\n",
        "    model.add(dropout(dropout_rate))\n",
        "\n",
        "    for i in range(n_dense - 1):\n",
        "        model.add(Dense(dense_units, kernel_initializer=kernel_initializer))\n",
        "        model.add(Activation(activation))\n",
        "        model.add(dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GbNiVkiq0Ovo",
        "colab": {}
      },
      "source": [
        "network1 = {\n",
        "    'n_dense': 6,\n",
        "    'dense_units': 16,\n",
        "    'activation': 'relu',\n",
        "    'dropout': Dropout,\n",
        "    'dropout_rate': 0.5,\n",
        "    'kernel_initializer': 'glorot_uniform',\n",
        "    'optimizer': 'sgd'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MgeRwZ7m0Ru3",
        "colab": {}
      },
      "source": [
        "network2 = {\n",
        "    'n_dense': 6,\n",
        "    'dense_units': 16,\n",
        "    'activation': 'selu',\n",
        "    'dropout': AlphaDropout,\n",
        "    'dropout_rate': 0.1,\n",
        "    'kernel_initializer': 'lecun_normal',\n",
        "    'optimizer': 'sgd'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CqHepCjI0TMH",
        "outputId": "ea55894e-cd4b-4a10-e308-ce575d3cf680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n",
        "                                                         test_split=0.2)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "8982 train sequences\n",
            "2246 test sequences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c_NsafCQ0V34",
        "outputId": "3feb9dd4-030c-40f8-f373-514ec7205277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "num_classes = np.max(y_train) + 1\n",
        "print(num_classes, 'classes')\n",
        "\n",
        "print('Vectorizing sequence data...')\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
        "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "46 classes\n",
            "Vectorizing sequence data...\n",
            "x_train shape: (8982, 1000)\n",
            "x_test shape: (2246, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a51I-Gm50f-I",
        "outputId": "2114434e-809d-4f9b-e381-867968fbcfde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print('Convert class vector to binary class matrix '\n",
        "      '(for use with categorical_crossentropy)')\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
            "y_train shape: (8982, 46)\n",
            "y_test shape: (2246, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m31eMlLX0iQw",
        "outputId": "d1ad1374-70c2-48f6-8898-643f699d6cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('\\nBuilding network 1...')\n",
        "\n",
        "model1 = create_network(num_classes=num_classes, **network1)\n",
        "history_model1 = model1.fit(x_train,\n",
        "                            y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            verbose=1,\n",
        "                            validation_split=0.1)\n",
        "\n",
        "score_model1 = model1.evaluate(x_test,\n",
        "                               y_test,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Building network 1...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/40\n",
            "8083/8083 [==============================] - 3s 346us/step - loss: 3.3034 - acc: 0.3298 - val_loss: 2.7531 - val_acc: 0.3315\n",
            "Epoch 2/40\n",
            "8083/8083 [==============================] - 2s 287us/step - loss: 2.6839 - acc: 0.3501 - val_loss: 2.5462 - val_acc: 0.3315\n",
            "Epoch 3/40\n",
            "8083/8083 [==============================] - 2s 291us/step - loss: 2.5597 - acc: 0.3522 - val_loss: 2.4776 - val_acc: 0.3315\n",
            "Epoch 4/40\n",
            "8083/8083 [==============================] - 2s 286us/step - loss: 2.4695 - acc: 0.3495 - val_loss: 2.3577 - val_acc: 0.3315\n",
            "Epoch 5/40\n",
            "8083/8083 [==============================] - 2s 298us/step - loss: 2.3729 - acc: 0.3587 - val_loss: 2.2314 - val_acc: 0.3315\n",
            "Epoch 6/40\n",
            "8083/8083 [==============================] - 2s 298us/step - loss: 2.3016 - acc: 0.3927 - val_loss: 2.1690 - val_acc: 0.4894\n",
            "Epoch 7/40\n",
            "8083/8083 [==============================] - 2s 295us/step - loss: 2.2521 - acc: 0.4156 - val_loss: 2.1316 - val_acc: 0.4894\n",
            "Epoch 8/40\n",
            "8083/8083 [==============================] - 2s 292us/step - loss: 2.2166 - acc: 0.4467 - val_loss: 2.1036 - val_acc: 0.5061\n",
            "Epoch 9/40\n",
            "8083/8083 [==============================] - 2s 295us/step - loss: 2.2008 - acc: 0.4727 - val_loss: 2.0971 - val_acc: 0.5050\n",
            "Epoch 10/40\n",
            "8083/8083 [==============================] - 2s 281us/step - loss: 2.1759 - acc: 0.4848 - val_loss: 2.0749 - val_acc: 0.5117\n",
            "Epoch 11/40\n",
            "8083/8083 [==============================] - 2s 286us/step - loss: 2.1655 - acc: 0.4934 - val_loss: 2.0556 - val_acc: 0.5195\n",
            "Epoch 12/40\n",
            "8083/8083 [==============================] - 2s 293us/step - loss: 2.1456 - acc: 0.4961 - val_loss: 2.0785 - val_acc: 0.5083\n",
            "Epoch 13/40\n",
            "8083/8083 [==============================] - 2s 283us/step - loss: 2.1433 - acc: 0.4926 - val_loss: 2.0741 - val_acc: 0.5117\n",
            "Epoch 14/40\n",
            "8083/8083 [==============================] - 2s 287us/step - loss: 2.1140 - acc: 0.5045 - val_loss: 2.0636 - val_acc: 0.5172\n",
            "Epoch 15/40\n",
            "8083/8083 [==============================] - 2s 288us/step - loss: 2.1073 - acc: 0.5070 - val_loss: 2.0463 - val_acc: 0.5195\n",
            "Epoch 16/40\n",
            "8083/8083 [==============================] - 2s 284us/step - loss: 2.0857 - acc: 0.5123 - val_loss: 2.0620 - val_acc: 0.5195\n",
            "Epoch 17/40\n",
            "8083/8083 [==============================] - 2s 285us/step - loss: 2.0880 - acc: 0.5131 - val_loss: 2.0489 - val_acc: 0.5184\n",
            "Epoch 18/40\n",
            "8083/8083 [==============================] - 2s 288us/step - loss: 2.0920 - acc: 0.5084 - val_loss: 2.0545 - val_acc: 0.5128\n",
            "Epoch 19/40\n",
            "8083/8083 [==============================] - 2s 305us/step - loss: 2.0746 - acc: 0.5169 - val_loss: 2.0506 - val_acc: 0.5184\n",
            "Epoch 20/40\n",
            "8083/8083 [==============================] - 2s 289us/step - loss: 2.0881 - acc: 0.5127 - val_loss: 2.0542 - val_acc: 0.5172\n",
            "Epoch 21/40\n",
            "8083/8083 [==============================] - 2s 282us/step - loss: 2.0644 - acc: 0.5159 - val_loss: 2.0412 - val_acc: 0.5172\n",
            "Epoch 22/40\n",
            "8083/8083 [==============================] - 2s 294us/step - loss: 2.0640 - acc: 0.5171 - val_loss: 2.0593 - val_acc: 0.5195\n",
            "Epoch 23/40\n",
            "8083/8083 [==============================] - 2s 290us/step - loss: 2.0592 - acc: 0.5196 - val_loss: 2.0497 - val_acc: 0.5206\n",
            "Epoch 24/40\n",
            "8083/8083 [==============================] - 2s 288us/step - loss: 2.0472 - acc: 0.5205 - val_loss: 2.0581 - val_acc: 0.5217\n",
            "Epoch 25/40\n",
            "8083/8083 [==============================] - 2s 297us/step - loss: 2.0522 - acc: 0.5200 - val_loss: 2.0580 - val_acc: 0.5172\n",
            "Epoch 26/40\n",
            "8083/8083 [==============================] - 2s 294us/step - loss: 2.0535 - acc: 0.5252 - val_loss: 2.0590 - val_acc: 0.5206\n",
            "Epoch 27/40\n",
            "8083/8083 [==============================] - 2s 295us/step - loss: 2.0439 - acc: 0.5247 - val_loss: 2.0484 - val_acc: 0.5228\n",
            "Epoch 28/40\n",
            "8083/8083 [==============================] - 2s 289us/step - loss: 2.0621 - acc: 0.5191 - val_loss: 2.0390 - val_acc: 0.5228\n",
            "Epoch 29/40\n",
            "8083/8083 [==============================] - 2s 287us/step - loss: 2.0518 - acc: 0.5200 - val_loss: 2.0506 - val_acc: 0.5195\n",
            "Epoch 30/40\n",
            "8083/8083 [==============================] - 2s 286us/step - loss: 2.0465 - acc: 0.5208 - val_loss: 2.1049 - val_acc: 0.5206\n",
            "Epoch 31/40\n",
            "8083/8083 [==============================] - 2s 289us/step - loss: 2.0430 - acc: 0.5199 - val_loss: 2.0448 - val_acc: 0.5206\n",
            "Epoch 32/40\n",
            "8083/8083 [==============================] - 2s 294us/step - loss: 2.0418 - acc: 0.5212 - val_loss: 2.1072 - val_acc: 0.5217\n",
            "Epoch 33/40\n",
            "8083/8083 [==============================] - 2s 289us/step - loss: 2.0355 - acc: 0.5223 - val_loss: 2.0572 - val_acc: 0.5217\n",
            "Epoch 34/40\n",
            "8083/8083 [==============================] - 2s 304us/step - loss: 2.0361 - acc: 0.5227 - val_loss: 2.0435 - val_acc: 0.5206\n",
            "Epoch 35/40\n",
            "8083/8083 [==============================] - 2s 298us/step - loss: 2.0294 - acc: 0.5255 - val_loss: 2.0475 - val_acc: 0.5217\n",
            "Epoch 36/40\n",
            "8083/8083 [==============================] - 2s 287us/step - loss: 2.0276 - acc: 0.5270 - val_loss: 2.0339 - val_acc: 0.5228\n",
            "Epoch 37/40\n",
            "8083/8083 [==============================] - 2s 284us/step - loss: 2.0253 - acc: 0.5259 - val_loss: 2.0315 - val_acc: 0.5217\n",
            "Epoch 38/40\n",
            "8083/8083 [==============================] - 2s 295us/step - loss: 2.0247 - acc: 0.5249 - val_loss: 2.0545 - val_acc: 0.5217\n",
            "Epoch 39/40\n",
            "8083/8083 [==============================] - 2s 286us/step - loss: 2.0215 - acc: 0.5268 - val_loss: 2.0513 - val_acc: 0.5161\n",
            "Epoch 40/40\n",
            "8083/8083 [==============================] - 2s 290us/step - loss: 2.0031 - acc: 0.5281 - val_loss: 2.0428 - val_acc: 0.5206\n",
            "2246/2246 [==============================] - 0s 87us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rKASWpvg0lZY",
        "outputId": "47a44605-a2b1-4f3e-c6ed-b84b654b5235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('\\nBuilding network 2...')\n",
        "model2 = create_network(num_classes=num_classes, **network2)\n",
        "\n",
        "history_model2 = model2.fit(x_train,\n",
        "                            y_train,\n",
        "                            batch_size=batch_size,\n",
        "                            epochs=epochs,\n",
        "                            verbose=1,\n",
        "                            validation_split=0.1)\n",
        "\n",
        "score_model2 = model2.evaluate(x_test,\n",
        "                               y_test,\n",
        "                               batch_size=batch_size,\n",
        "                               verbose=1)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Building network 2...\n",
            "Train on 8083 samples, validate on 899 samples\n",
            "Epoch 1/40\n",
            "8083/8083 [==============================] - 3s 416us/step - loss: 2.7966 - acc: 0.3024 - val_loss: 2.5832 - val_acc: 0.3315\n",
            "Epoch 2/40\n",
            "8083/8083 [==============================] - 3s 338us/step - loss: 2.3766 - acc: 0.3775 - val_loss: 2.1424 - val_acc: 0.4605\n",
            "Epoch 3/40\n",
            "8083/8083 [==============================] - 3s 336us/step - loss: 2.1653 - acc: 0.4436 - val_loss: 1.9621 - val_acc: 0.5083\n",
            "Epoch 4/40\n",
            "8083/8083 [==============================] - 3s 336us/step - loss: 2.0299 - acc: 0.4841 - val_loss: 1.9068 - val_acc: 0.5506\n",
            "Epoch 5/40\n",
            "8083/8083 [==============================] - 3s 337us/step - loss: 1.9504 - acc: 0.5079 - val_loss: 1.8446 - val_acc: 0.5628\n",
            "Epoch 6/40\n",
            "8083/8083 [==============================] - 3s 347us/step - loss: 1.8942 - acc: 0.5207 - val_loss: 1.8339 - val_acc: 0.5673\n",
            "Epoch 7/40\n",
            "8083/8083 [==============================] - 3s 344us/step - loss: 1.8528 - acc: 0.5260 - val_loss: 1.8314 - val_acc: 0.5729\n",
            "Epoch 8/40\n",
            "8083/8083 [==============================] - 3s 332us/step - loss: 1.8256 - acc: 0.5312 - val_loss: 1.7969 - val_acc: 0.5818\n",
            "Epoch 9/40\n",
            "8083/8083 [==============================] - 3s 342us/step - loss: 1.7776 - acc: 0.5498 - val_loss: 1.7827 - val_acc: 0.5740\n",
            "Epoch 10/40\n",
            "8083/8083 [==============================] - 3s 335us/step - loss: 1.7585 - acc: 0.5521 - val_loss: 1.7852 - val_acc: 0.5873\n",
            "Epoch 11/40\n",
            "8083/8083 [==============================] - 3s 337us/step - loss: 1.7315 - acc: 0.5576 - val_loss: 1.8034 - val_acc: 0.5784\n",
            "Epoch 12/40\n",
            "8083/8083 [==============================] - 3s 346us/step - loss: 1.7112 - acc: 0.5638 - val_loss: 1.7517 - val_acc: 0.5962\n",
            "Epoch 13/40\n",
            "8083/8083 [==============================] - 3s 341us/step - loss: 1.6972 - acc: 0.5638 - val_loss: 1.7795 - val_acc: 0.5918\n",
            "Epoch 14/40\n",
            "8083/8083 [==============================] - 3s 348us/step - loss: 1.6583 - acc: 0.5771 - val_loss: 1.7503 - val_acc: 0.5940\n",
            "Epoch 15/40\n",
            "8083/8083 [==============================] - 3s 357us/step - loss: 1.6545 - acc: 0.5822 - val_loss: 1.7557 - val_acc: 0.6085\n",
            "Epoch 16/40\n",
            "8083/8083 [==============================] - 3s 353us/step - loss: 1.6439 - acc: 0.5804 - val_loss: 1.7242 - val_acc: 0.6018\n",
            "Epoch 17/40\n",
            "8083/8083 [==============================] - 3s 356us/step - loss: 1.6250 - acc: 0.5899 - val_loss: 1.7349 - val_acc: 0.5996\n",
            "Epoch 18/40\n",
            "8083/8083 [==============================] - 3s 336us/step - loss: 1.6046 - acc: 0.5895 - val_loss: 1.7294 - val_acc: 0.6151\n",
            "Epoch 19/40\n",
            "8083/8083 [==============================] - 3s 342us/step - loss: 1.5931 - acc: 0.5946 - val_loss: 1.6863 - val_acc: 0.6196\n",
            "Epoch 20/40\n",
            "8083/8083 [==============================] - 3s 333us/step - loss: 1.5758 - acc: 0.5979 - val_loss: 1.7119 - val_acc: 0.6151\n",
            "Epoch 21/40\n",
            "8083/8083 [==============================] - 3s 345us/step - loss: 1.5771 - acc: 0.6025 - val_loss: 1.6767 - val_acc: 0.6174\n",
            "Epoch 22/40\n",
            "8083/8083 [==============================] - 3s 339us/step - loss: 1.5735 - acc: 0.6036 - val_loss: 1.7091 - val_acc: 0.6118\n",
            "Epoch 23/40\n",
            "8083/8083 [==============================] - 3s 342us/step - loss: 1.5538 - acc: 0.6058 - val_loss: 1.6908 - val_acc: 0.6085\n",
            "Epoch 24/40\n",
            "8083/8083 [==============================] - 3s 335us/step - loss: 1.5295 - acc: 0.6108 - val_loss: 1.6538 - val_acc: 0.6429\n",
            "Epoch 25/40\n",
            "8083/8083 [==============================] - 3s 349us/step - loss: 1.5343 - acc: 0.6097 - val_loss: 1.6810 - val_acc: 0.6263\n",
            "Epoch 26/40\n",
            "8083/8083 [==============================] - 3s 333us/step - loss: 1.5072 - acc: 0.6178 - val_loss: 1.6452 - val_acc: 0.6307\n",
            "Epoch 27/40\n",
            "8083/8083 [==============================] - 3s 343us/step - loss: 1.5077 - acc: 0.6188 - val_loss: 1.6965 - val_acc: 0.6274\n",
            "Epoch 28/40\n",
            "8083/8083 [==============================] - 3s 348us/step - loss: 1.5053 - acc: 0.6173 - val_loss: 1.6620 - val_acc: 0.6229\n",
            "Epoch 29/40\n",
            "8083/8083 [==============================] - 3s 349us/step - loss: 1.4868 - acc: 0.6245 - val_loss: 1.6438 - val_acc: 0.6418\n",
            "Epoch 30/40\n",
            "8083/8083 [==============================] - 3s 340us/step - loss: 1.4849 - acc: 0.6202 - val_loss: 1.6797 - val_acc: 0.6329\n",
            "Epoch 31/40\n",
            "8083/8083 [==============================] - 3s 331us/step - loss: 1.4731 - acc: 0.6250 - val_loss: 1.6521 - val_acc: 0.6418\n",
            "Epoch 32/40\n",
            "8083/8083 [==============================] - 3s 336us/step - loss: 1.4505 - acc: 0.6340 - val_loss: 1.6395 - val_acc: 0.6418\n",
            "Epoch 33/40\n",
            "8083/8083 [==============================] - 3s 334us/step - loss: 1.4480 - acc: 0.6339 - val_loss: 1.6063 - val_acc: 0.6440\n",
            "Epoch 34/40\n",
            "8083/8083 [==============================] - 3s 334us/step - loss: 1.4442 - acc: 0.6374 - val_loss: 1.7089 - val_acc: 0.6429\n",
            "Epoch 35/40\n",
            "8083/8083 [==============================] - 3s 332us/step - loss: 1.4349 - acc: 0.6426 - val_loss: 1.6579 - val_acc: 0.6363\n",
            "Epoch 36/40\n",
            "8083/8083 [==============================] - 3s 333us/step - loss: 1.4292 - acc: 0.6369 - val_loss: 1.6466 - val_acc: 0.6485\n",
            "Epoch 37/40\n",
            "8083/8083 [==============================] - 3s 330us/step - loss: 1.4103 - acc: 0.6453 - val_loss: 1.6253 - val_acc: 0.6440\n",
            "Epoch 38/40\n",
            "8083/8083 [==============================] - 3s 340us/step - loss: 1.4111 - acc: 0.6409 - val_loss: 1.6366 - val_acc: 0.6485\n",
            "Epoch 39/40\n",
            "8083/8083 [==============================] - 3s 335us/step - loss: 1.4020 - acc: 0.6458 - val_loss: 1.6392 - val_acc: 0.6496\n",
            "Epoch 40/40\n",
            "8083/8083 [==============================] - 3s 337us/step - loss: 1.3932 - acc: 0.6493 - val_loss: 1.6750 - val_acc: 0.6652\n",
            "2246/2246 [==============================] - 0s 99us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X53AE4pC0zbQ",
        "outputId": "0354227f-ce68-47bc-9535-f712ce167296",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print('\\nNetwork 1 results')\n",
        "print('Hyperparameters:', network1)\n",
        "print('Test score:', score_model1[0])\n",
        "print('Test accuracy:', score_model1[1])\n",
        "print('Network 2 results')\n",
        "print('Hyperparameters:', network2)\n",
        "print('Test score:', score_model2[0])\n",
        "print('Test accuracy:', score_model2[1])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Network 1 results\n",
            "Hyperparameters: {'n_dense': 6, 'dense_units': 16, 'activation': 'relu', 'dropout': <class 'keras.layers.core.Dropout'>, 'dropout_rate': 0.5, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'sgd'}\n",
            "Test score: 2.059647047190611\n",
            "Test accuracy: 0.5329474621814803\n",
            "Network 2 results\n",
            "Hyperparameters: {'n_dense': 6, 'dense_units': 16, 'activation': 'selu', 'dropout': <class 'keras.layers.noise.AlphaDropout'>, 'dropout_rate': 0.1, 'kernel_initializer': 'lecun_normal', 'optimizer': 'sgd'}\n",
            "Test score: 1.6467193957428155\n",
            "Test accuracy: 0.650044523650583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "md8FHCIH0_PI",
        "outputId": "d9369f64-28fd-41c5-f73e-99bd9d50a808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plt.plot(range(epochs),\n",
        "         history_model1.history['val_loss'],\n",
        "         'g-',\n",
        "         label='Network 1 Val Loss')\n",
        "plt.plot(range(epochs),\n",
        "         history_model2.history['val_loss'],\n",
        "         'r-',\n",
        "         label='Network 2 Val Loss')\n",
        "plt.plot(range(epochs),\n",
        "         history_model1.history['loss'],\n",
        "         'g--',\n",
        "         label='Network 1 Loss')\n",
        "plt.plot(range(epochs),\n",
        "         history_model2.history['loss'],\n",
        "         'r--',\n",
        "         label='Network 2 Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd4lUX2wPHvpCeQBAihBpIISjGE\nIARpAiIKWBCxIEqzLCoWsPxcu66rri6uqIgiCiIKK66IgoAi0kGQIr13gqEF0ntyfn9MKiQhCblp\nnM/zvM/NfevcEN5z552ZM0ZEUEoppS7EqaILoJRSqmrQgKGUUqpYNGAopZQqFg0YSimlikUDhlJK\nqWLRgKGUUqpYNGAopZQqFg0YSimlikUDhlJKqWJxqegClKW6detKUFBQRRdDKaWqjA0bNpwWEf/i\n7FutAkZQUBDr16+v6GIopVSVYYw5XNx99ZGUUkqpYtGAoZRSqlg0YCillCqWatWGoZQ6X1paGhER\nESQnJ1d0UVQF8vDwICAgAFdX11KfQwOGUtVcREQE3t7eBAUFYYyp6OKoCiAiREVFERERQXBwcKnP\no4+klKrmkpOT8fPz02BxCTPG4Ofnd9G1TA0YSl0CNFiosvgb0IChlFKqWDRgAHf+707eWP5GRRdD\nqWrLGMPTTz+d8/7dd9/ltddeK/KYpUuXsnr16jIvy9SpU3nssceK3GfXrl107twZd3d33n333QL3\nue+++/j000/zrfvhhx/o169fkecOCgri9OnTxV5fmWjAAPZE7WFNxJqKLoZS1Za7uzvff/99iW6I\njggY6enpxdqvTp06fPjhhzzzzDOF7jN48GC++eabfOu++eYbBg8efFFlrMwcFjCMMR7GmD+MMZuN\nMduNMf8oYJ+njDE7jDFbjDG/GWMC82zLMMZsylrmOKqcAEG1gjgcU+zR8UqpEnJxcWHkyJGMGzfu\nvG2nTp3i9ttvJzw8nPDwcFatWsWhQ4eYOHEi48aNIywsjGXLlhEcHIyIEB0djbOzM8uXLwege/fu\n7N27lzNnzjBgwABCQ0Pp1KkTW7ZsAeC1115j6NChdO3alaFDh+a79rx58+jcufN5gaxevXqEh4cX\n2QX1uuuuY9euXURGRgKQkJDAokWLGDBgAAADBgygffv2XHnllUyaNKlUv7fCPtOyZcsICwsjLCyM\ndu3aERcXR2RkJN27dycsLIyQkBBWrFhRqmsWxZHdalOAXiISb4xxBVYaYxaISN6v8n8CHUQk0Rjz\nCPBvYFDWtiQRCXNg+XIE+gay5OASREQbB1W1NubnMWw6vqlMzxnWIIz3+75/wf0effRRQkNDefbZ\nZ/OtHz16NE8++STdunXjyJEj9OnTh507d/Lwww9Ts2bNnG/5LVq0YMeOHRw8eJCrrrqKFStWcPXV\nV3P06FEuv/xyHn/8cdq1a8cPP/zA4sWLGTZsGJs22c+6Y8cOVq5ciaenJ1OnTgVg9uzZvPfee8yf\nP5/atWuX+HM7Oztz++238+233zJ69Gjmzp1Lz5498fHxAWDKlCnUqVOHpKQkwsPDuf322/Hz8yvR\nNV599dUCP9O7777LhAkT6Nq1K/Hx8Xh4eDBp0iT69OnDiy++SEZGBomJiSX+TBfisIAhIgLEZ711\nzVrknH2W5Hm7BhjiqPIUJdA3kLjUOKKTo6ntWfI/HKXUhfn4+DBs2DA+/PBDPD09c9YvWrSIHTt2\n5LyPjY0lPj7+vOOvueYali9fzsGDB3n++ef57LPP6NGjB+Hh4QCsXLmSWbNmAdCrVy+ioqKIjY0F\noH///vmuuXjxYtavX8/ChQtzbvClMXjwYJ555hlGjx7NN998k68G8+GHHzJ79mwAjh49yt69e0sc\nMAr7TF27duWpp57i3nvvZeDAgQQEBBAeHs79999PWloaAwYMICys7L9vO3TgnjHGGdgANAcmiMja\nInZ/AFiQ572HMWY9kA68LSI/OKqcIfVC6BHYg/jUeA0YqlorTk3AkcaMGcNVV13Ffffdl7MuMzOT\nNWvW4OHhUeSx3bt355NPPuGvv/7i9ddfZ+zYsSxdupRrrrnmgtetUaNGvvfNmjXjwIED7Nmzhw4d\nOpTuwwBdunQhMjKSzZs3s3r16pw2jaVLl7Jo0SJ+//13vLy86NmzZ5mOtH/uuee46aabmD9/Pl27\nduWXX36he/fuLF++nHnz5jFixAieeuophg0bVmbXBAc3eotIRtZjpQCgozEmpKD9jDFDgA7A2Dyr\nA0WkA3AP8L4xplkhx440xqw3xqw/depUqcrZp3kflo5YShPfJqU6XilVPHXq1OGuu+5i8uTJOetu\nuOEGxo8fn/M++zGSt7c3cXFxOes7duzI6tWrcXJywsPDg7CwMD799FO6d+8O2BrI9OnTAXvDrlu3\nbqG1h8DAQGbNmsWwYcPYvn17qT+PMYZBgwYxfPhw+vXrlxP0YmJiqF27Nl5eXuzatYs1a0rXqaaw\nz7R//37atGnD3//+d8LDw9m1axeHDx+mfv36/O1vf+PBBx9k48aNpf5chSmXXlIiEg0sAfqeu80Y\n0xt4EegvIil5jjmW9XoAWAq0K+Tck0Skg4h08Pcv1hwgSqkK9PTTT+drZP7www9Zv349oaGhtG7d\nmokTJwJwyy23MHv2bMLCwlixYgXu7u40adKETp06AfZmGhcXR5s2bQDbuL1hwwZCQ0N57rnn+PLL\nL4ssR8uWLZk+fTp33nkn+/fvz7ft+PHjBAQE8N577/HGG28QEBCQ83jrXIMHD2bz5s35ekf17duX\n9PR0WrVqxXPPPZdT5gsJDQ0lICCAgIAAnnrqqUI/0/vvv09ISAihoaG4urrSr18/li5dStu2bWnX\nrh0zZ85k9OjRxbpmSRjb1FD2jDH+QJqIRBtjPIGFwDsi8lOefdoB3wF9RWRvnvW1gUQRSTHG1AV+\nB24VkR0UoUOHDlLaCZQ6TOpAn2Z9ePO6N0t1vFKV1c6dO2nVqlVFF0NVAgX9LRhjNmQ9zbkgR7Zh\nNAS+zGrHcAK+FZGfjDGvA+tFZA72EVRN4H9ZvZOOiEh/oBXwqTEmM+vYty8ULC5WQloCu6J2OfIS\nSilVpTmyl9QWCniMJCKv5Pm5dyHHrgbaOKpsBQn0DeRwtI7FUEqpwuhI7yyBvoE6eE8ppYqgASNL\nUK0gTieeJiE1oaKLopRSlZIGjCzhjcMZEjqEpPSkii6KUkpVSjrjXpbel/Wm92UFNqkopZRCaxj5\niAhpGWkVXQylqp2qlt58+vTphIaG0qZNG7p06cLmzZvP20fTm1/CMiWTumPr8sqSVy68s1KqRKpa\nevPg4GCWLVvG1q1befnllxk5cuR5+2h680uYk3HC191Xe0op5QBVLb15ly5dcjLYdurUiYiIiPPK\nrenNL3GBtQI5FH2ooouhlOOMGQObyja9OWFh8H71TW8+efLkAh8zaXrzS1xQrSAW7l9Y0cVQqlqq\niunNlyxZwuTJk1m5cmWB2zW9+SUs0DeQyLhIUjNScXN2q+jiKFX2ilETcKSqlN58y5YtPPjggyxY\nsKDQG72mN7+E9QruxXPdniM1I7Wii6JUtVRV0psfOXKEgQMH8tVXX3HFFVcU+nk0vfklrHtgd966\n7i1qutWs6KIoVW1VhfTmr7/+OlFRUYwaNYqwsLAiJ1nS9OZV1MWkNwc7DiM6ORpAZ95T1YamN1fZ\nLja9udYw8kjLTMPv3358sPaDii6KUkpVOhow8nBzdqORdyMdi6GUUgXQgHGOwFo6L4ZSShVEA8Y5\nAn118J5SShXEYQHDGONhjPnDGLPZGLPdGPOPAvZxN8bMNMbsM8asNcYE5dn2fNb63caYPo4q57kC\nfQM5GnuUjMyM8rqkUkpVCY4cuJcC9BKReGOMK7DSGLNARPJ2SH4AOCsizY0xdwPvAIOMMa2Bu4Er\ngUbAImPMFSLi8Lv4gJYDCKwVSIZk4Iyzoy+nlFJVhsNqGGJlj+93zVrO7cN7K5DdWfo74DpjjMla\n/42IpIjIQWAf0NFRZc3r6oCrebjDwzrSW6kyVNXSm+/atYvOnTvj7u7Ou+++W+h+VSEleVlyaBuG\nMcbZGLMJOAn8KiJrz9mlMXAUQETSgRjAL+/6LBFZ6xwuIzODrSe2ciz2WHlcTqlLQlVLb16nTh0+\n/PDDnMSHynJowBCRDBEJAwKAjsaYkLK+hjFmpDFmvTFm/alTpy76fMnpyYRODGXa5mllUDqlFFS9\n9Ob16tUjPDwcV1fXEn/WypaSvCyVS/JBEYk2xiwB+gLb8mw6BjQBIowxLoAvEJVnfbaArHUFnXsS\nMAnsSO+LLWsNtxrU9aqrYzFUtdVzas/z1t115V2MCh9FYloiN06/8bztI8JGMCJsBKcTT3PHt3fk\n27Z0xNJiXbeqpjcvqcqWkrwsOSxgGGP8gbSsYOEJXI9t1M5rDjAc+B24A1gsImKMmQPMMMa8h230\nvhz4w1FlPZd2rVWq7FXF9OalUdlSkpclR9YwGgJfGmOcsY++vhWRn4wxrwPrRWQOMBn4yhizDziD\n7RmFiGw3xnwL7ADSgUfLo4dUtsBagew4tePCOypVBRVVI/By9Spye12vusWuURSkKqU3L2sVlZK8\nLDmyl9QWEWknIqEiEiIir2etfyUrWCAiySJyp4g0F5GOInIgz/FvikgzEWkhIgscVc6CBPkGcTj6\nMNUpMaNSlUFVSW9+MSpbSvKypCO9CzA8bDjf3vktmZJZ0UVRqtqpCunNjx8/TkBAAO+99x5vvPEG\nAQEBOY+3zlXZU5KXJU1vrlQ1p+nNVTZNb+4AyenJzN87n/1n9l94Z6WUukRowChAUloSN824iR92\n/VDRRVFKqUpDA0YBannUwtvNW8diKKVUHhowCmCMIahWkAYMpZTKQwNGIXQiJaWUyk8DRiECfQO1\nhqGUUnlowCjEk52eZOnwpTp4T6kyUNXSm0+fPp3Q0FDatGlDly5d2Lx5c4H7aXpzBUCzOs1o26At\ndnoOpdTFqGrpzYODg1m2bBlbt27l5ZdfZuTIkWVajqpKA0YhYpJj+HT9p+w8tbOii6JUlVfV0pt3\n6dIlJ4Ntp06diIiIKPZn1fTml6Dk9GQenvcw4/uNp5W/jpJV1UjPnuevu+suGDUKEhPhxvPTmzNi\nhF1On4Y78qc3Z+nSYl22qqY3nzx5Mv369SvWZwRNb35JqlejHh4uHtpTSqkyUhXTmy9ZsoTJkyez\ncuXKYn9OTW9+CTLG0NS3qfaUUtVPUTUCL6+it9etW+waRUGqUnrzLVu28OCDD7JgwQL8/PyK8emK\npunNqzmdSEmpslVV0psfOXKEgQMH8tVXX3HFFVeU6DNqevPqbvFiKKDKqWMxlCp7VSG9+euvv05U\nVBSjRo0iLCysyEmWNL15FVXq9OZt2kBQEMydm2/1ifgTuDq7UsezTtkUUKkKoOnNVbaLTW/uyDm9\nmwDTgPqAAJNE5INz9vk/4N48ZWkF+IvIGWPMISAOyADSi/uBSiU0tMAaRv2a9R12SaWUqmoc+Ugq\nHXhaRFoDnYBHjTGt8+4gImNFJExEwoDngWUicibPLtdmbXfspLuhoXDkCERH51t9Iv4ELy1+ia0n\ntjr08kopVRU4ck7vSBHZmPVzHLATaFzEIYOB/zqqPIVJTEvkK8ka9r81f2BIyUjhzRVvsiZiTXkX\nS6kyVZ0ePavSKYu/gXJp9DbGBAHtgLWFbPcC+gKz8qwWYKExZoMxxmHj8j1cPPgwaal9kzUiM1sj\n70Y4G2ftKaWqNA8PD6KiojRoXMJEhKioqAt2Xb4Qh4/DMMbUxAaCMSJS8CzqcAuw6pzHUd1E5Jgx\nph7wqzFml4gsL+D8I4GRAE2bNi1x+ZyME727DaPDw2OZN+hm8rZauDi5EOAToD2lVJUWEBBAREQE\np06dquiiqArk4eFBQEDARZ3DoQHDGOOKDRbTReT7Ina9m3MeR4nIsazXk8aY2UBH4LyAISKTgElg\ne0mVppxDw4bx9up3+GbfD4yum79bm06kpKo6V1dXgoODK7oYqhpw2CMpY9O8TgZ2ish7ReznC/QA\nfsyzroYxxjv7Z+AGYJujytravzVDk1tQ+9V/QWZmvm2BtQI5mXDSUZdWSqkqw5E1jK7AUGCrMWZT\n1roXgKYAIjIxa91twEIRSchzbH1gdlZqcRdghoj87MCyMtSEcf3Cmexdv5DLO/bNWf/pzZ/i7uzu\nyEsrpVSV4LCAISIrgQtOJiEiU4Gp56w7ALR1SMEKcVWfEfCvmayZPylfwPBwubhGIqWUqi40NUgW\nv/DuZBo4ueY3MiX3sdSBswcY/sNwNh3fVMTRSilV/WnAyOblRXxgQ4KOxLL00NKc1W7Obny1+Sum\n/Dml4sqmlFKVgAaMPGpc1Ymm8c58teWrnHUBPgE80uERJqybwJ+Rf1Zg6ZRSqmJpwMjDefoMJo4f\nznc7viMxLXfmqzd6vYGfpx+j5o/K97hKKaUuJRow8vLwYGjbocSnxvPjrpxevtT2rM3Y68eyJmIN\nX2/5ugILqJRSFUdn3MsrLo4ez09kpGddpm2ZxuA2g3M2DWs7jLjUOAa2GliBBVRKqYqjNYy8atTA\n/DSP4WeDWLh/Icfjj+dsMsbwWMfHqOlWUx9LKaUuSRow8nJygjZtCDtpyJRM/rv1/OS5O0/t5MqP\nr9QMtkqpS44GjHOFhuK1cx8dGrZn2pZp520O8AkgNiWWUfNGkZGZUQEFVEqpiqEB41yhoXD2LI80\n7M+m45vYdjJ/Citvd2/G9RnHn8f/5JP1n1RQIZVSqvxpwAAQgZQU+3O7dtCuHQP8r8HFyYWvNn91\n3u53tr6T6y+7nhcXv5ivnUMppaozDRhpaeDnB2++ad937gwbN1Kn07X0bd6X6Vunn/foyRjDRzd+\nRHJ6MhP+mFABhVZKqfKnAcPVFfz9Ydv52dOHhQ7jWNwxlhxact62K/yuYMV9K3it52vlUEillKp4\nGjAAQkJg+/bc9y+8AN26cUuLW/B1982XKiSvjo074uzkTFRiFCnpKeVUWKWUqhgaMMAGjH37ICnJ\nvndygjVr8Mgw3Nn6TmbtmEVCakKBhx6PP07LCS0Z+dNInTNZKVWtacAAGzAyM2HXLvs+NBQyMmDn\nToa1HUZCWgJP/vJkgd1oG9RswOMdH2fa5mmMXT22nAuulFLlRwMG2IABue0YoaH2dcsWujXtxvPd\nnuezjZ8x6LtBJKcnn3f4y91f5u6Qu3lu0XP8sOuHciq0UkqVL0fO6d3EGLPEGLPDGLPdGDO6gH16\nGmNijDGbspZX8mzra4zZbYzZZ4x5zlHlBKB5c9v4nR0wmjcHDw/YsgVjDG9d9xbj+oxj1s5Z9Jve\nj5jkmHM/B1P6T6FDow7c+/29bD2x1aHFVUqpiuDIGkY68LSItAY6AY8aY1oXsN8KEQnLWl4HMMY4\nAxOAfkBrYHAhx5YNV1do2TK34dvFBR56CK68MmeXMZ3G8PVtX7PyyEp6ftnzvPEXnq6e/Hj3jwxp\nM4Tg2sEOK6pSSlUUhwUMEYkUkY1ZP8cBO4HGxTy8I7BPRA6ISCrwDXCrY0qaJSQkf9fa99+H++7L\nt8u9ofcyd/Bc9kTtoeuUruw7sy/f9obeDfn0lk+p6VaT+NT4Ah9fKaVUVVUubRjGmCCgHbC2gM2d\njTGbjTELjDHZX+kbA0fz7BNBIcHGGDPSGLPeGLP+1KlTpS9kSAgcPgyxsbnr4uMhNTXfbn2b92Xx\nsMXEJMfQdUpXNkZuPO9UqRmp9JjagwfmPKA9p5RS1YbDA4YxpiYwCxgjIrHnbN4IBIpIW2A8UOIW\nYxGZJCIdRKSDv79/6Qua3fC9Y4d9XbUKfHxg2bLzdr064GpW3r8SDxcPek7tyeKDi/Ntd3N2445W\ndzBj6wzeWvFW6cuklFKViEMDhjHGFRsspovI9+duF5FYEYnP+nk+4GqMqQscA5rk2TUga53jZAeM\n7HaMK66wOaa2FtyA3bJuS1bdv4qmvk3pN70fm49vzrf9uW7PMSR0CC8teYlZO2Y5suRKKVUuHNlL\nygCTgZ0i8l4h+zTI2g9jTMes8kQB64DLjTHBxhg34G5gjqPKCkBQEHh55bZj+PtDw4awZUuhhwT4\nBLB0xFJ83X0Z+dPIfOM0jDF8dstndA7ozLAfhmnPKaVUlefIGkZXYCjQK0+32RuNMQ8bYx7O2ucO\nYJsxZjPwIXC3WOnAY8Av2Mbyb0Vke0EXKTNOTtC6df6G79DQIgMGQF2vuozrM44/jv1xXrpzDxcP\nvh/0Pd0Du+Pl6uWIUiulVLkx1alRtkOHDrJ+/frSn+C+++DnnyEy0r5/9ln44ANISLBdbQshIvSd\n3pffj/7Ozkd30tin4M5g2b/rrEqVUkpVOGPMBhHpUJx9daR3XiEhcPw4REXZ9wMHwn/+Y1OgF8EY\nw8c3fkxaZhqPL3i8wH1S0lO467u7eGfVO2VdaqWUKhfFChjGmGbGGPesn3saY54wxtRybNEqwLkN\n3506wWOPgafnBQ9tVqcZr/Z4ldm7ZvPjrh/P2+7m7IarkysvLn6RRQcWlWWplVKqXBS3hjELyDDG\nNAcmYXswzXBYqSpK9sjuvO0Y+/fnT31ehKc7P02bem14bMFjxKXE5duW3Qjeqm4rBs8azJGYI2VV\naqWUKhfFDRiZWQ3RtwHjReT/gIaOK1YFadwYfH3zB4wBA+Dvfy/W4a7Orky6ZRLHYo/x0uKXztte\nw60G3w/6npT0FO749g6dQ0MpVaUUN2CkGWMGA8OBn7LWuTqmSBXImPNThBSjp1RenQI68UiHRxj/\nx3jWHVt33vYr/K7gywFfcij6EHui9pRFqZVSqlwUN2DcB3QG3hSRg8aYYKDgaeiquuyAkd17rG1b\nOHoUzp4t9ineuu4tGtRswMifRpKemX7e9tta3ca+J/bRpn6bsiq1Uko5XLEChojsEJEnROS/xpja\ngLeIVM/uPiEhNjgcz8pGm2dujOLy9fBlfL/xbDq+iffXvF/gPj7uPmRKJm8uf7PAfFRKKVXZFLeX\n1FJjjI8xpg42/9NnxpgCR29XeedOptShgx3Ut3BhiU4zsNVA+rfoz6tLX+VQ9KEC94lOjmbihonc\nNOMmtp3cVuA+SilVWRT3kZRvVuLAgcA0Ebka6O24YlWgc3tK1a0Lc+fCM8+U6DTGGD7q9xFOxolR\n80YVmLW2jmcdfhnyC07GiR5TexTY5qGUUpVFcQOGizGmIXAXuY3e1ZO/P9Srl7/h+8YboXbtEp+q\niW8T/nntP1mwb0GhU7e29m/NivtW4Ovuy3XTrmP54eWlLblSSjlUcQPG69i8TvtFZJ0x5jJgr+OK\nVcHO7SkFMHkyvFfyp3CPdXyMkHohPPnLkySmJRa4z2W1L2PFfSsIqhXE6cTTpSmxUko5XHEbvf8n\nIqEi8kjW+wMicrtji1aBQkLsvBiZmbnrfvsN3nzzvAmVLsTFyYUJN07gcMxh3l75dqH7NfZpzMaH\nNjKw1UAAjsU6Npu7UkqVVHEbvQOMMbONMSezllnGmABHF67ChITY2faO5BmNPWQInDljkxOWUPfA\n7tzT5h7+verf7D+zv9D9XJxsgsOlh5bS7MNmfPHnFyW+llJKOUpxH0l9gZ2PolHWMjdrXfVUUIqQ\n66+3DeDTp5fqlGOvH4ursyujfx59wX3DG4XTPbA798+5n/Frx5fqekopVdaKGzD8ReQLEUnPWqYC\nFzEfaiVXUMBwdYVBg2DOnPzzfhdTI+9GvNbjNebtncfc3XOL3LeGWw3mDp7LgJYDeOLnJ3jql6dI\nSksq8TWVUqosFTdgRBljhhhjnLOWIdiZ8aonX19o0uT8pINDhkB4eO6gvhJ64uonaO3fmtE/jyY5\nPbnIfd1d3Pnfnf9jVIdRjFszju92fFeqayqlVFkpbsC4H9ul9jgQiZ0pb4SDylQ5FNRTqlMnWL7c\nzvddCq7OrnzU7yMORh/k36v+fcH9XZxcmHDTBP548A+GhA4BYNWRVcSnxpfq+kopdTGK20vqsIj0\nFxF/EaknIgOAIntJGWOaGGOWGGN2GGO2G2POe3hvjLnXGLPFGLPVGLPaGNM2z7ZDWes3GWMuYhq9\nUgoJgZ07If38XFBERUFMTKlOe23wtQy6chD/WvkvDp49WKxjwhuHY4whLiWOm/97M20+acNvB34r\n1fWVUqq0LmbGvacusD0deFpEWgOdgEeNMa3P2ecg0ENE2gD/xM61kde1IhJW3OkDy1RICKSk2Pkw\n8oqMhIYN4fPPS33qd294F2fjzJO/PFmi47zdvZk7eC5uzm70/qo3I+eOJCa5dIFLKaVK6mICRpET\nU4tIpIhszPo5DtgJND5nn9Uikp0Gdg1QebrqFtTwDTZYtG1b6t5SAAE+Abzc/WV+3P0jC/YuKNGx\n3Zp2Y9NDm3i2y7NM/nMybT5pQ0JqQqnLopRSxXUxAeP85EiFMMYEAe2AtUXs9gCQ9+4pwEJjzAZj\nzMgizj3SGLPeGLP+1KlTxS3ShbVqZefHKGi2vSFD4M8/7SOrUnqy85O08GvBEz8/UeKJlDxdPXnn\n+ndY88Aa7m1zLzXcagAwd/dcbd9QSjlMkQHDGBNnjIktYInDjse4IGNMTewUr2OyEhgWtM+12ICR\nd2q7biJyFdAP+zire0HHisgkEekgIh38/cuwp6+XFzRrdn4NA2z3Wieni6pluDm7Mb7fePad2ccb\ny98o1TnCG4fzr97/AuBozFFu/eZWmo5ryqtLXiUqsfp2YlNKVYwiA4aIeIuITwGLt4i4XOjkxhhX\nbLCYLiLfF7JPKPA5cKuI5NzlRORY1utJYDbQsfgfq4wU1FMKoEEDO5BvxozciZZK4fpm1zMkdAhv\nrHiD0QtGFzjZUnE18W3C6gdW0z2wO68vf52m7zflyZ+f5FRCGda6lFKXtIt5JFUkY4wBJgM7RaTA\nrH3GmKbA98BQEdmTZ30NY4x39s/ADUD5TxgREgJ79tjG73P9+9/w66/2sdVF+OLWL3iy05N8+MeH\n3Dj9Rs4mFX9mv3N1CujED3f/wPZR27mj9R18/ufnZEgGALN3zmbCHxNYd2ydziWulCoVU9A8DWVy\nYmO6ASuArUB2Fr8XgKYAIjIsg/hJAAAgAElEQVTRGPM5tnvu4azt6SLSISsb7uysdS7ADBF580LX\n7NChg6xfX4Y9cL/5BgYPhs2bc2fec5Apf07h4Z8eJrh2MHPunkOLui0u+pwxyTH4evgCcPd3dzNz\n+0zAPg5rW78tvYJ78XbvwhMiKqWqP2PMhuL2RHVYwKgIZR4wtm2DNm3so6fBg8/fvnYtfPSRTX3u\n5nbRl1t5ZCUDZw4kNSOVb+/8lhua3XDR58wmIhyNPcofx/5g3bF1/PHXH3i7eTNn8BwAnlv0HK3q\ntuLWlrdSy6NWmV1XKVW5acAoK6mpUKMGPPusTW1+rnnz4OabbX6pW24pk0seij5E///2Z/up7bx3\nw3s8cfUTmIt87HUhiWmJtJ7QmsMxh3F1cqX3Zb25o/Ud3NriVvy8/Bx6baVUxSpJwHBYG0a14OYG\nLVoU3PANcMMNF5XBtiBBtYJY/cBqbrniFsb8MoaRc0eSmlGyOThKysvVi4OjD7L2wbWM6TSGnad3\n8sCcB5ixdQYA+8/s59UlrzJt8zRWH13NifgTBU45q5Sq3rSGcSF33w3r1p0/4jvbY4/ZR1KRkVCr\n7B7lZEomryx5hTdXvEnzOs158ZoXGRI6JGfODEcSETZGbiSwViB1veoyZ/ccbpt5G5mSO6FUTbea\nLByykM5NOnM68TSZkkm9GvUcXjalVNnSGkZZCgmBAwcgoZDR1H/7m+1F9a9/lellnYwTb/R6g3n3\nzMPbzZv7fryPFh+1YMqfU0jLSCvTa53LGEP7Ru2p61UXgP4t+pP4QiK7Ht3FvHvm8WHfD3mg3QME\n1QoCYNKGSdR/tz5tJ7bl6V+eZv7e+TqAUKlqSGsYFzJ7NgwcaGsZHQoJwk8+aR9dPfxw2V47i4gw\nd89cXl/2OhsiNxBcK5gXrnmB4W2H4+rs6pBrlsTOUzv5cfePLDqwiJVHVpKSkUJNt5pEPRuFm7Mb\n/1n9H3ae3om7szseLh54uHjQyLsRj3Z8tKKLrtQlTxu9y9KhQxAcDG+/DX//+wV3dyQRYf7e+fxj\n2T9Y99c6An0DeeGaFxgaOhRPV88KLVu2pLQkVh9dzf6z+xnZ3mZ0Gf7DcH478BvJ6ck5Syv/Vmx9\nZCsAryx5hdD6ofRv0R8354vvbaaUKj4NGGWtZ084fBj27QNn54L3yciAL7+04zUKq4mUERHh530/\n849l/2DtsbX4uPtwV+u7GB42nK5Nujq8V1VZyMjMwNnJmcS0RFpNaMWRmCP4e/kzNHQoD1z1AK39\nz01srJRyBA0YZe1//4O77oK5c2032oLExUHz5nZypeXLL3oEeHGICMsOL2Pqpql8t+M7EtISaFa7\nGcPaDmNY22E5bQyVXUZmBgv3L2Tyn5P5cfePpGem8+WALxnWdhh/xf3FgbMHqFejHvVr1MfH3adK\nBESlqgoNGGUtLQ2CgmztYUER6cg//dS2Y3z/Pdx2W9mXowjxqfHM2jGLLzd/yZJDSwDoEdiDwSGD\naVanGfVq1KNejXrU9apbLj2tSutkwkm+2vwVQ0KHUL9mfSZtmMRDPz2Us93d2Z16NeqxePhimtdp\nTmRcJK7OrjkN9KrqWhuxlsj4SAa0HFDiY6MSo3j+t+fpGdSTwSGD9UtFCWjAcIR//ANeew327rU1\niYKkp9u5MlJTbVr0Mhj9XRqHow/z1ZavmLZ5GnvP7M23zWDw8/LL+cYeXCuYPs37cEOzGyrlCO/I\nuEi2ntzKifgTnEg4wcmEk5xIOMG4PuOo41mHp395mnFrxtGxcUf6Ne/HjZffSPtG7XEyJe8AKCIk\npCWQkZmRk1JFOV56Zjr/XPZP3ljxBpmSyUPtH+KDvh/g7uJerON3nNrBLf+9hQNnDwC2V9/EmybS\n0LuhI4tdbWjAcITISGjaFJ54Av7zn8L3mz8fbroJPvjA7luBRIR9Z/ZxPP54zs32ZMJJTsSf4GSi\nfd1xagdnk8/ibJzp1rQbN11+EzddcROt6raqEt/Stp7Yyuxds5m/dz5/HPsDQWjh14Kdj+7EGMPb\nK9/maMxR29iekUxKegqh9UN5pccrAHT/ojsHow8SmxJLXEocgjA4ZDAzbreDFof/MJyWfi3p2rQr\n4Y3CC+xckJ12ZcuJLWw5sYX7291Pg5oNiEmOwck44e3uXa6/k6rkUPQh7v3+XlYfXc2wtsNoUKMB\n/179bzoHdOa7u76jkXfRsyjM3zufu7+7Gy9XL74f9D1rI9bywuIX8HDx4IO+HzA0dGiV+DuuSBow\nHGXQIFi4EI4ds/NlFEQEHnkEBgyAvn0dV5Yykp6ZztqItczbO495e+ex5cQWwI44v+nym+jXvB/X\nBF6Dj7tPsc+578w+5u6ey6KDiwDwdfe1i4cvPu4+OT/X8axDcK1ggmsH4+HicdGf5XTiaX7Z9wvR\nydE5XXY7fd6J/Wf35+vS2z2wOx/f9DEAj81/jKS0JHzcffB298bH3YeQeiH0bd6XmOQYOk3uxK7T\nuwBwdXKlfaP2/F+X/2Ngq4FsjNzI6J9Hs/XEVmJScqfK/WXIL9zQ7AbeXf0uLy5+kd6X9WZAiwH0\nb9Gf+jXrX/TnrC5mbpvJQz89hCB8ctMn3NPmHgD+t/1/3PfjfXi7ezPrrll0adLlvGNFhP/8/h+e\n/fVZwhqE8ePdP9LEtwkAe6P2cv+c+1l5ZCU3XX4Tn978KY19Gp93DmVpwHCU5cuhRw87n/cDDzju\nOhXoaMxR5u+dz7y981h0YBFJ6Uk4G2fCG4fTK6gX1wZfS5cmXfByzQ2YGZkZrD22ljm75zBn9xx2\nnrYzEbaq2wovVy9iUmKISY4hJiWmwDQnBkOATwDN6zTPt7Ss25KWdVuW6vFSemY6zsa5TL5dRiVG\nsfroalYdXcWqo6t4ouMT3Hnlnew+vZsH5z5Im3ptCK0fSmj9UELqheQE1z8j/+TrLV8ze9dsDkYf\nxGDoEdSDRUMX4ezkzKojq4hNicXJOGGMwWCo41mH9o3aAxARG4Gniye+Hr6Vut2ppOJT43l8weNM\n3TSVTgGdmDFwBsG1g/Pts+3kNgZ8M4AjMUcY3288I9uPzPm3TElP4aGfHuLLzV9yZ+s7+eLWL3Jm\nncyWKZl89MdHPLfoOdyc3RjXZxwjwkZobaMAGjAcRcQ2fLu6woYNRfeEio+3c2aMGAGXXea4MjlQ\ncnoyq4+uZsnBJSw+tJi1EWvJkAzcnN3oHNCZHoE9OBJ7hJ/2/MTpxNO4OLnQI7AH/Vv055Yrbjnv\nJpB9zuzgEZUYxYGzB9h3Zh/7zu5j/5n97Duzj1OJuZM++br70rlJZ7o26UrXJl3p2LhjgTeH3ad3\ns+6vdTmZeDcd34S7szsh9UIIqRdCm3pt7Gv9NgU2kIsIsSmxRCVFEZUYRWxKLF6uXjm1Dm83b7zd\nvQu8cadlpBGfGk9cahzxqfHEp8bj5erFFX5X4Obshoiw9eRWftj1AyfiTzDhpgkAtJ/Uno2RG/Od\nK9A3kFb+rajpVpPfDvzG2WQ7P0oN1xrU9qzNzZffzCc3fwLYlPjOxpkabjVIz0znr7i/OBpzlNNJ\np4vM9SUIaRlpJKUlkS7ppGek57w6GScaeDegee38wbtBzQYF3myPxhxl+eHlLD+8nHV/rWPtg2tx\ndXbl43Ufs+zwMprVbmaXOvb1ePxx7v3+Xvad2ceL17zIKz1eKXTw6dmks9z7/b0s2LeAB9s9yEc3\nfsTZ5LMMnDmQ3yN+5x89/8HL3V8uMgjsO7OPB+Y8wPLDy7mh2Q3c1fou2jZoy5X+V1aasUvJ6cms\n/2s9BkMNtxp4uXpRwzXr1a2Gw8cmacBwpOyeUKtXQ+fOhe937JjtYnvzzTBzpmPLVE7iUuJYeWQl\niw8uZvGhxfwZ+Se+Hr7cePmN9L+iP32a9ymThvOY5Bj2n93P1hNbc77Zbz9l51Z3Ns6ENQijS5Mu\neLp4su6vdWyI3EBsip39t6ZbTdo3bE+HRh1ISU9h68mtbD25lTNJZ3LOX79GfVr7tyY9Mz0nQEQl\nRRVrxkNPF0983H1wd3HPCQ6FJYd0cXKhhV+LnKCVvQTXCiYmJYbpW6azJmINfx7/kz1Re8iQDAyG\nkHohpGSkcODsgfPKVNOtJs1qN+Nkwkki4yPPu6arkyuNfRojIjk5vjIkg0zJJFMyqeVRi9oetcmU\nTA5GHzzv+OBawTg7OXPgzAEyycx33obeDbnS/0pa+7cmJjmGOXvmcDLhJGADe7em3ZjcfzL1a9bn\nn8v+yeQ/JxMRG5EziZezccZgaODdgE6NO/FX/F/U8axjF486BNUKYnSn0QAsP7ychNQEPF09mb5l\nOp//+Tlh9cOISoridOJpPu//OXe0vgMXJ5cL1kAzJZOP133MS4tfynl06GScuLzO5bRt0JbQeqG0\nbdCWy2pfRlJakm3PSo3LadeKTYklNiUWdxf3nNpkcO3gUtV8s51KOMW8vfOYs3sOC/cvJCGtkNRD\n2L8jbzdvAnwCaOLbhCY+WYtv7muAT0CpH+tqwHCk+Hho3NimM//666L3ffVVeP31CweXKio2JRZP\nF89ySU9yNuksayLW5ASQtcfWkpaRRliDMMIbhRPeOJyOjTvSwq8Fzk75B1eKCMfjj7P15Fa2ndzG\n1pNb2XV6F+7O7vh5+VHXsy5+Xn74efrlvPp6+JKYlphzwzj3BpKd/iTv4u3mnfNzTEoM209uZ9up\nbWw7uS2nBw+Ah4sHyenJgJ3M6urGV3NN02u4JvAaujTpkvNIK1MyOR5/nEPRhzgUfYiDZw9yKPoQ\nxxOOU8+rHnU862CMISU9heiUaCLjbJfUUeGjSEhNIPyzcGp71qa2R+2c1/4t+tP7st7EpsTy5aYv\ncXFywdXZFRcnF1ycXAhvFE6Lui3YdnIbzyx8hv1n9/NX3F8kpiUC0Ni7MVFJUTnlz1bHow7BtYOp\nV6MeR2KOsPfM3nyB1NvNm4beDenbrC+v9nyVj/74iOWHl3Mm6UzOElgrMGf0f9cpXVl9dHW+azgZ\nJxp5N2LO3XMY8eOInPY2g8HFyYWeQT1ZOHQhAA/OeZCk9CT8vfypV6Me/l7+ObXLLSe2MHPbTA5E\nH+Bw9OF8NdrCOBknRATB3i9rutXM9ygytH4oAT4BeLp45rSVuTm75av97Dq9K+ex7eqjqxGExt6N\n6d+iP32b98XDxYPEtEQSUhNISEvI+TkxLZHo5Ggi4iI4GnOUo7FHOZ14Ol/5anvU5szfz1AalSJg\nGGOaANOA+oAAk0Tkg3P2McAHwI1AIjBCRDZmbRsOvJS16xsi8uWFrlkuAQNg9GiYOBGOHoV6RWRo\njY+Hyy+3vauWLwf34nUTVBeWnplOpmRWmVQi8anx7Dy1k20nt7H91HbqeNbhmqbXEN44vEwa/B0t\nJjmGQ9GHCKwViK+7LycTTuYEsuzlcMxhIuMjaerblBZ+LWjh14KWdVvSom6LYo2TyR79D7bhOiop\nivjUeBJSE4hPjSdDMri1xa34evgy5c8pHI8/TnpmOumZ6aRlpNGibgtGhI0A4MbpN7Inag8nE04S\nlxoHwN0hd/Pf2/8LgPe/vM9LkNkruBejrx6Nl4sXfab3QUSo41mHejXq0bBmQwaHDCa0QSjrjq3j\nk/WfcDb5LFGJUaRkFD7lsZuTG27Objg5OeXUgkPrhRLeOJzugd1pVbcVTsYJZydngmoFUcujFhmZ\ntkZY1BexpLQkImIjOBp7NKcX4EMdHip0/6JUloDREGgoIhuz5ufeAAwQkR159rkReBwbMK4GPhCR\nq40xdYD1QAdssNkAtBeRIie8LreAsXs3tGxpJ1V64YWi950506ZIHznSPs5SSpWr5PRkTieeJiMz\ng8BagYAdu2EwOBmnnBu2r7svfl5+pGak8vnGz3O6oGd3SR905SAev/pxTieeps0nbUjNSM233NHq\nDq4NvpaI2Aj+tfL87NV3tb6LsTeM5WzSWcI+DTtv+9RbpzI8bDirjqyi2xfdcHVyzWnT8PP0Y1yf\ncVx32XWISJk23leKgHHehYz5EfhIRH7Ns+5TYKmI/Dfr/W6gZ/YiIg8VtF9hyi1gAFx/vQ0cBw6A\nywV6sEyeDN2729qGUqrayX5c5WScSMtI46+4v0jJSMmXcLNZ7WY09mlMbEosq4+uzmlXypRMMjIz\naN+oPU19m3I4+jBfb/na1q7SEkhITSAyPpK3e79NaP1QZm6byQuLX6B7YHe6N+1O98DuXFb7slIH\nkZIEjHLpq2eMCQLaAWvP2dQYOJrnfUTWusLWVx6PPmrTf/z0kx1zUZTsLrgiNl36bbeVS64ppVT5\nyO4WDeDq7JpTkymIj7sPfZsXPkYrsFYgL3Z/sdDt/jX8aVu/LXN3z2XqpqkArL5/NZ2bOL6d1OEB\nwxhTE5gFjBGRWAecfyQwEqBp06ZlffrC3XwzNGkCEyZcOGBk++EHuP12+L//g3fe0aChlCqxXsG9\n6BXci0zJZOepnSw7vIyrGl5VLtd26Ix7xhhXbLCYLiLfF7DLMaBJnvcBWesKW38eEZkkIh1EpIO/\nv3/ZFLw4XFxs99pFi+yjqeIYMABGjYKxY237h1JKlZKTceLKelcyKnxUsfNuXfQ1HXXirB5Qk4Gd\nIvJeIbvNAYYZqxMQIyKRwC/ADcaY2saY2sANWesqlwcftAkGP/64ePsbA+PHw7Bh8PLL8P77ji2f\nUkqVIUfWMLoCQ4FexphNWcuNxpiHjTHZc5nOBw4A+4DPgFEAInIG+CewLmt5PWtd5VKvHtx5J0yd\nCvv3F+8YJyfbCH777XYGv8OHHVpEpZQqKzpw72Jt3Wp7QGVk2G6zgwcX77jUVNi4ETp1cmz5lFKq\nCCXpJeXQNoxLQps2sGmTzTF1zz1w//2QUPgw/xxubrnBYupUePxxSCl8AJBSSlU0DRhlITAQli6F\nl16yN/8OHWDz5uIfv28ffPQRdOlif1ZKqUpIA0ZZcXGBf/7T9pqKiYGrr7ZdbovzyO+NN+DHH+Hg\nQbjqqmqTrFApVb1owChrvXrZ2sV118Fjj8HAgXCmGO31/fvbR1shITaVyJ9/Or6sSilVAhowHMHf\nH+bOhffeg3nzoF07GwwupGlTWLbM1jbatbPr4uIcW1allComDRiO4uQETz4Jq1ZBZiZ07QqzZl34\nOFdXW9sA+OMP2z7y0UeQlubY8iql1AVowHC08HBYt872orrjDjs/RnG7MjdubNs0Hn/cPqqaPbv4\nxyqlVBnTgFEeGjSAJUvsCO9XX4VBgyAx8cLHNW4Mv/4Kc+bYGsvAgdCvnwYNpVSF0IBRXjw8bJfb\nsWPhu++gWzc7AdOFGGNn99u61Q4M7NvXrhMp3vFKKVVGNGCUJ2PgmWdsg/i+ffZx1e+/F+9YFxc7\nCdOYMfb93LnQrJmd/e/g+XMzK6VUWdOAURFuugnWrIGaNaFnT3j+efjvf21bx9kiJxXM1bEj3Hef\nbRC/7DK45hpbA9HGcaWUg2guqYoUFQVDhsDPP+dfX6cONG9uZ+hr1gzq1rWPtDw9c5fs93FxsH49\nfP21bRc5cMC2d2zZAi1a6DziSqkiVboZ91Qh/PxgwQJISrI3+n37YO9e+7pvH6xcCTNmXLiR+4or\n4KmnbIO4k5OtZfTubV8HDbIN7Q0bls9nUkpVW1rDqOxSU20tIinJLsnJuT8nJcFff9n5ODZssOnW\nH3/ctnVs2mRrHd9+a2sj//63nb/DSZ9CKqVylaSGoQGjOhCxyQ/HjrU1Fi8vO4/4k09Cejo89JDt\n1vv772WXTj0lxT72at4catcum3Mqpcqdpje/1BgD114L8+fb7rd33gkTJ9qb+XPPwa23wocfQuvW\ndv/ffit5KvWEBHvcq6/aa9WqZRveW7a0qUyUUtWe1jCqq2PH4IMPYNo0OHEid32TJhARYRvSn3kG\nbr7ZPvbK+7gr7+vu3bB8uX3klZ5uH2m1a2cnjWrXzubL2rQJRoywU876+lbYR1ZKlVyleCRljJkC\n3AycFJGQArb/H3Bv1lsXoBXgLyJnjDGHgDggA0gv7ofRgFEAEYiMtDf1TZtsFtxVq+y64nBzszWJ\n7t1t190uXcDHJ3d7aqpN6/7WWxAQYAcnXntt0efcswe++gpWr7YDEYcNg/r1S/0RlVKlV1kCRncg\nHphWUMA4Z99bgCdFpFfW+0NABxE5XZJrasAogchI28bxww92etnvvrOPmTZsAGdnmyyxfn2bedfD\n48LnW7PG3vj37rWDCf/1L9vtN9uZM3aej2nT7L5OTrbb8O7ddlDizTfbdpe+fe17pVS5qBQBI6sg\nQcBPxQgYM4AlIvJZ1vtDaMAoH2lpsGuXnWoWoEcP+wgq+9FT375w773QqtWFz5WYaNtMxo+3Y0Cm\nTIHTp22QmDvX1kauvBKGD7fT2TZuDDt3wuTJdp9Tp6BRI/t46/777RiUbCkpNuhEReW+Nmpkaz/G\nOORXo9SloEoFDGOMFxABNBeRM1nrDgJnAQE+FZFJxbmeBowykJICa9fC4sV2WbUK+vSxDepgb9R+\nfkWf47ff7E0/IsK+9/e3QWfYMAgLK/gGn5oKP/1kg8fPP9uU8C1b2sb2M2cKnyc9LAxGjbIBqEaN\nUn9spS5VVS1gDAKGiMgtedY1FpFjxph6wK/A4yKyvJDjRwIjAZo2bdr+8OHDZfgJFMePQ3S0vXkf\nPWrTkPToYUeoDxyYvz0jr+ho+OwzWzPp08fO81FcERHw5Zc2VUqtWjZA1amT/7V2bTvCfcIE273X\n19emSnnkETuQsSjx8XDokH3U5udnj9XxKeoSVdUCxmzgfyIyo5DtrwHxIvLuha6nNQwHO3HC3qCn\nT7cj0z087FS0775rA0pFELG1oAkT7ARVaWlw/fXw6KO2R1jekfPZy/Hj+c/h5GQDUN6A1LgxPPGE\nfYSmVDVWZQKGMcYXOAg0EZGErHU1ACcRicv6+VfgdRH5uaBz5KUBo5yI2IbrGTPs46Ply23qka+/\nhkWL7A27d+/y7/l0/Dh8/rkdg3LsWP5tjRrZcSnZS3CwDS5RUblLdttIVJQNNElJ8PDD8Nprthuy\nUtVQpQgYxpj/Aj2BusAJ4FXAFUBEJmbtMwLoKyJ35znuMmB21lsXYIaIvFmca2rAqGDvvmt7R505\nY9+HhtquuB99ZN+np5dPD6j0dPjlF9se07y5bTwvaftGVJQdpDhxInh7259HjbLdjAuTmGjbev73\nP9sGM2aM7W1WEseOwTff2Em3une3tSTleJGRdgDq8OH5e/ddAipFwKgIGjAqgYwMO9bj119to7mI\nrXWAHZ9x9KhtqA4Lgw4dbG3E2bliy1yU7dttYseFC23byHvvwY035jbcJyfbWtbMmbYnWEKCzemV\nnm4DZ9eutufYjTcW3k4iYsekjB9vuzdnZORuCwqygaNHD/varFnF9go7fty2KxWnq3VVkZZmf7dr\n1tjefV9+CVdfXdGlKjclCRiISLVZ2rdvL6oSGzdO5I47RJo3F7G3SZHbb6/oUl1YZqbIvHkiLVrY\nMt9wg8iMGSJDh4r4+Nh1fn4iI0eK/PabSFqaSHy8yAcfiDRtardfeaXItGkiqam5501KEpk6VeSq\nq+w+vr4iTz8tsnevyMaNIu+/LzJwoEjdurm/r4YNRQYPFlm9unx/BwkJIqNH2zLUry/y5psiZ86U\nbxkc5bnn7Od6/nmRJk1EnJzsz8nJFV2ycgGsl2LeYyv8Jl+WiwaMKiQ2VmTmTJFFi+z76GiRuXPt\nzbmySk21N/Fatex/nVq1RO6/X+SXX/IHgnOPmTZNJCTEHtO0qQ2cL70k4u9v17VuLTJxog0yBcnM\nFNmxw+5zzz02OIFI794iy5YVv/wRESLvvivy1lsip04V/7jffxe54gp7zb/9TaRPH/tzjRo2iBw6\nVPxzFSYzU+TXX0W+/FLk7NmLP19x/fKL/SwjR9r30dH23xRE2rSxgbuyy8wUOXmy1IdrwFBVz9ix\n9s/x6qvtjaMyB47Tp0WWLxdJSSn+MZmZIj/9JNKtm/2cxoj0728DZkk/a3y8vfHXr2/P1aOHrdkU\ndJ7YWFuL6d3bXjO7puLlZW/2R44Ufp3kZPtN28lJJDDQXiPb5s22huXiIuLsbGs9pbm5ZmSIfPed\nSPv2uWVzdxe56y77BaKwQFwWIiNF6tWztb+EhPzbfvpJpEED+/n+8Q/HlqO0MjPt76hzZ5FmzWzN\nthQ0YKiqJzVV5LPP7COB7Jtg3m/Pf/1lv3mW8j9FpbJpk8iBAxd/nsRE+9irUSP7O+vaNbe2M2+e\nvYl7etptl10m8sorInv2iGzfLjJsmL3Ru7rab9S7d+c/98aN9hs2iDzwgEhMTMFlOHLEPkbz9rb7\n9upla1AbNoikpxde9tRUG8hatrTHNW8u8vnnImvWiDz+eO5jOH9/G9g2bCjbLxEZGSLXX29/P9u2\nFbxPVJSt0YENaCtXFl4LLE9pafaRaGioLVtgoMhHH5XsC0weGjBU1ZWcLDJ+vP12d8cduetr15ac\nb6Cenvab4ZgxudunTLHf1g8dKvpGVR0lJYlMmJAbbL287GudOiKPPGLbOwq62R46JPLYYyIeHrb2\nceedIn/8IfL66/abdcOG9pt2cURHi7zzjv2mm/3v5OMj0q+ffQS2cqX9t01MtP++2W07bduKfPPN\n+f9mKSkiP/5o/wbc3CTn0d0zz4h88YUtZ1xc6X9nb79tzzlp0oX3nTUr9/EhiAQEiFx3nf3djhsn\nMn++yL59jv+7S0625c3+HbdqZR/hXWTtpyQBQ3tJqcopMdGO+M4etT1lih09Hh9vZyCMi7M9kIYO\nhdjY/GnV3dzsiPQxY+zkUdl/49U951Rqqs0W/McfcMstdsreoroBZztxwqbCnzDB/i7BploZP94O\nZCypiAhYscIuy5fbnmaQOw/92bM26/GLL9oyXujf5exZO3Pk11/b0f9553IJDLSDK6+8EkJC7Pn8\n/Ys+3++/2+7et99uu/bqsaAAAA5lSURBVDAX5+8iKsqmvNm71ybM3LPHvkZH5+7j5wc33GDzr/Xp\nU7JxSGlp9m8+72yaeWfX3LQJxo2zM2x26AAvvGDnuSmDDAXarVZdWkTs+IW8o7n37MlNX7Jzpx1I\n2LNn7tK8efUPICUVE2PTzgcF2ezBZeX0aTs//YoVtlvuQw/ZG3Zpfv/p6TbLwPbt+Zfdu23AdHe3\n/+5jxtgAcq6zZ21STWPsTfhi5m8RsZ9t9+7ceWN+/hlOnrTbr7rKBo9+/exMl8bYbuV5A07265Ej\nF77etdfC88/bv+Uy/NvVgKFUXtu3wxtv2Glss9OCNGpkU7uHh9v/+Bo8qrb0dNi2zQ60nDbNfivv\n3dum8O/b134TF7GzUf74ow1gjhhrkZlpA9HPP9vpkn//3Y6r8fa2AS1v7cjHx477uOIK+wXG1ze3\nFubpmf9nf3+Hpd/RgKFUQUTsN7qlS+2gwokTbQ6psWNtfqzrr7fLNddccqN9q5WoKJg0yWYY+Osv\ne1MePdo+4nnqKXjnHXj22fIpS3S0fZT1228220B2gGjRwg7wrARfVDRgKFUSM2fCp5/aJIbZjzX6\n9rU1ELCPtLy9bULCSvAfXBVTWppN0zJunM1sDLaNYcECzU6chwYMpUojIcE+h/7119yJn8A+f167\n1n5DzP522LUrPPZYxZZXFY9kpV6ZOxeefvrCjeKXGA0YSpWlFSvs8/Hsxs3du21PlW+/tdtvucU2\nFHftahdNGKiqkJIEDJ08WakLueYau+SVmWlfk5Jsd8gvvsjNytukiW0XGTTI9ooZONA2yqal5b6+\n8orNjHroEDz+uH3kVbOmffX2httug7Zty/VjKnUhGjCUKo3sZ+CenrZBMz3dzvy3alVuWwjYmQb9\n/e2ri0vua3YtJCnJjluIj88dY5KQYINF27a2h9f06dCrl629aGO8qkD6SEqpyiYlxTauu7nZdpT7\n77ddM93d7YC3666zMwrWqlXRJVXVQEkeSWlXAaUqG3f33BHaw4bZwWbz5tkgcfYsvPVW7kRUS5fa\nHkDV6Iufqry0hqFUVRMbawd9QW4PrqZNbVvJ7bfbWoiTk21n2bfPjgU4ezb3tVkzO95EKSpJDcMY\nM8UYc9IYs62Q7T2NMTHGmE1Zyyt5tvU1xuw2xuwzxjznqDIqVSVlBwuwNY8pU6BNG/j4Y9s4P2aM\n3ZaRYbsAX321HVdy993wyCN29DHYR1+//ZZ/hj+liuDIRu+pwEfAtCL2WSEi+ZLWGGOcgQnA9UAE\nsM4YM0dEdjiqoEpVWX5+cN99domNtQEke3ChqyvMmGEDTO3ats0jewE7//jAgTZNyj332BxM2jNL\nFcFhAUNElhtjgkpxaEdgn8j/t3f/QVbVZRzH3w/rIpigsNsguiCYDElFoEVRjuKPGgKNmlAxptGG\n1Jgsm7Rcp5nC0UZzrMSyHAzUScMs05RpVBKGUFNJBQQsQWVBQEAbJGYWUHn64/le72HdXU7L7j0X\n9vOaOXPP+d4f89zvzN1nv99zzvP1VwDM7B5gEqCEIdKevn3h/PP3bmt5nDV+fLkK7E03wY03wsiR\nUQvJLMprLFgQU1vu8dizJ8ydG++fMyde26tXeevTJ8616E7qg1LRl9WONbNlwEbgCndfCRwDrM+8\n5jWg+6zILlIpvXtHMb5zzok72++9d+9y3+vWlZNHjx6xZa/MevJJuO++qNG0c2e0DRsW95UAXHNN\nvPfUU2HMmDiZLwe0Lj3pnUYY89z9fXWGzawvsMfdd5jZBGCmuw8zs8nAeHf/Rnrd14BPuXurdRjM\n7GLgYoDBgwef1NTU1DVfRkTa5h43JO7aFaMMiHtHFi6M/V694gT9JZfEuRSIUU1tbYxaStvw4VEW\nXCrmgLjT2923Z/b/ama/NrN6YAOQra3QkNra+pxZwCyIq6S6KFwRaU/pvpHsgk0LFkTl2Mcfh0WL\nYlu8OBLGnj1Rerylyy+PhNHcHFd7DR8eZb1POKH8mGdRKOkShSUMMzsK2OzubmZjiCu23gS2AcPM\nbCiRKKYAXy0qThHZD3V1sTLcpEl7t5vFZb67d++9laa83norTsYvWRJTZaWZkOuug8bGSCg7dqiQ\nYIV1WcIws7nAOKDezF4DfgzUArj7rcBkYLqZvQM0A1PS+rLvmNmlwCNADTAnndsQkYOFWfur3R11\nVFzxBZEc1qyBVatiWgviuXPPhbFjo/jj2WfDiBEqP9/FdOOeiBx41qyJq7seegieey7ahg6NSsK1\ntTB7dkyJlVat69UrytNffXW89qmnYNOmcrHHPn3i0uOBA4v7TgU5IM5hiIh02PHHw4wZsW3YAPPm\nwSOPlEumrFsXd8CXruDauTMSSSlhzJwZV4Rl1dfD1q2xf+21UUl4yJDyduyx3b50vUYYItI9ZNdu\n37gRNm+O6sClrUePmOYCmDYtbmwsrQEPcNJJ5ZX7JkyIpFRXF1t9fdxRP21aZb9TJ9AIQ0Skpez5\njaOPjq0ts2fHY3NzJIa1a/d+ftSomOZ6882YBnviiThRX0oYF10U51TGjYu75w+SGxk1whAR6Qy7\nd8clv9u3x4qMq1dHe79+5Rpfp50WCaixMWp4lTaIJX/POCPev2pVTIENGNDlyUYjDBGRSivdH9K3\nL7z0UiyMtWhRlKBfvBhefTUSRnNzXC5cU1Pe3Mt3yy9ZAmeeWf7MwYMjeVx/fSSijRsjoQwaFNth\nh1XsKyphiIh0hYYGmDo1tqzhw8ujj9aMHh0n8ZuaYiqsqSm2mpp4fv58uPDC8uvr6yMxjRjR2d/g\nfZQwRESqSf/+MHFi28+fdVaMWtavj+mtdeti6qoClDBERA4kdXVR0LEAB8epexER6XJKGCIikosS\nhoiI5KKEISIiuShhiIhILkoYIiKSixKGiIjkooQhIiK5HFTFB81sK9DUwbfXA290YjidSbF1jGLr\nGMXWMQdqbMe6e661bg+qhLE/zOyfeSs2Vppi6xjF1jGKrWO6Q2yakhIRkVyUMEREJBcljLJZRQfQ\nDsXWMYqtYxRbxxz0sekchoiI5KIRhoiI5NLtE4aZjTezf5vZGjNrLDqeLDNba2YvmNlSMyt8sXIz\nm2NmW8xsRaatv5nNN7PV6bFfFcU2w8w2pP5bamYTCohrkJktNLNVZrbSzC5L7YX3WzuxVUO/9TKz\nZ8xsWYrt6tQ+1MyeTr/XP5hZzyqK7Q4zezXTb6MqHVsmxhoze97M5qXjzuk3d++2G1ADvAwcB/QE\nlgEjio4rE99aoL7oODLxnAKcCKzItN0ANKb9RuCnVRTbDOCKgvtsIHBi2u8DvASMqIZ+aye2aug3\nAw5P+7XA08CngXuBKan9VmB6FcV2BzC5yH7LxPg94PfAvHTcKf3W3UcYY4A17v6Ku+8G7gEmFRxT\n1XL3vwP/adE8Cbgz7d8JfKmiQSVtxFY4d9/k7s+l/f8CLwLHUAX91k5shfOwIx3Wps2B04E/pfai\n+q2t2KqCmTUAE4HfpmOjk/qtuyeMY4D1mePXqJIfTOLAo2b2rJldXHQwbRjg7pvS/utAZRYXzu9S\nM1uepqwKmS4rMbMhwGjiP9Kq6rcWsUEV9FuaVlkKbAHmE7MB29z9nfSSwn6vLWNz91K//ST12y/M\n7NAiYgNuAn4A7EnHdXRSv3X3hFHtTnb3E4EvAN8ys1OKDqg9HuPdqvlPC/gN8CFgFLAJ+FlRgZjZ\n4cB9wHfdfXv2uaL7rZXYqqLf3P1ddx8FNBCzAR8uIo7WtIzNzD4KXEXE+EmgP3BlpeMys7OALe7+\nbFd8fndPGBuAQZnjhtRWFdx9Q3rcAtxP/GiqzWYzGwiQHrcUHM973H1z+mHvAW6joP4zs1riD/Ld\n7v7n1FwV/dZabNXSbyXuvg1YCIwFjjSzQ9JThf9eM7GNT1N87u67gNsppt8+C3zRzNYSU+ynAzPp\npH7r7gljCTAsXUHQE5gCPFhwTACY2QfMrE9pH/g8sKL9dxXiQeCCtH8B8JcCY9lL6Q9y8mUK6L80\nfzwbeNHdf555qvB+ayu2Kum3D5rZkWm/N/A54hzLQmByellR/dZabP/K/ANgxDmCivebu1/l7g3u\nPoT4e7bA3afSWf1W9Nn8ojdgAnF1yMvAD4uOJxPXccRVW8uAldUQGzCXmKJ4m5gHnUbMjz4GrAb+\nBvSvoth+B7wALCf+QA8sIK6Tiemm5cDStE2ohn5rJ7Zq6LeRwPMphhXAj1L7ccAzwBrgj8ChVRTb\ngtRvK4C7SFdSFbUB4yhfJdUp/aY7vUVEJJfuPiUlIiI5KWGIiEguShgiIpKLEoaIiOSihCEiIrko\nYYjsg5m9m6lAutQ6saqxmQ3JVtgVqWaH7PslIt1es0cZCJFuTSMMkQ6yWK/kBos1S54xs+NT+xAz\nW5CK0D1mZoNT+wAzuz+to7DMzD6TPqrGzG5Lays8mu4exsy+k9aqWG5m9xT0NUXeo4Qhsm+9W0xJ\nnZd57i13/xjwK6JKKMAvgTvdfSRwN3Bzar8ZWOTuHyfW7liZ2ocBt7j7R4BtwFdSeyMwOn3ON7vq\ny4nkpTu9RfbBzHa4++GttK8FTnf3V1IRv9fdvc7M3iDKabyd2je5e72ZbQUaPIrTlT5jCFEee1g6\nvhKodfdrzexhYAfwAPCAl9dgECmERhgi+8fb2P9/7Mrsv0v53OJE4BZiNLIkU21UpBBKGCL757zM\n4z/S/pNEpVCAqcDitP8YMB3eW4DniLY+1Mx6AIPcfSGxrsIRwPtGOSKVpP9YRPatd1pdreRhdy9d\nWtvPzJYTo4TzU9u3gdvN7PvAVuDrqf0yYJaZTSNGEtOJCrutqQHuSknFgJs91l4QKYzOYYh0UDqH\n8Ql3f6PoWEQqQVNSIiKSi0YYIiKSi0YYIiKSixKGiIjkooQhIiK5KGGIiEguShgiIpKLEoaIiOTy\nPwlKHIMnBO04AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ-CIrgxbDLe",
        "colab_type": "text"
      },
      "source": [
        "Batch Normalization\n",
        "=====\n",
        "\n",
        "##1. Internal Covariance Shift  \n",
        "각 layer마다 distribution이 달라져 학습하는 데 어려움이 발생.  \n",
        "layer를 거칠 때마다 분포가 같으면 weight값만 조정해주면 되는데....\n",
        "이를 위해 layer의 distribution을 맞춰출 필요가 있음. \n",
        "\n",
        "\n",
        "##2. 배치 정규화는 각 층의 출력값들을 정규화하는 방법\n",
        "\n",
        "##3. Algorithm\n",
        "1. $\\mu$ = $\\frac 1 m$ $\\sum x_i$   \n",
        "  \n",
        "  \n",
        "2. $\\sigma_B^2$ = $\\frac 1 m$ $\\sum(x_i - \\mu_B)^2$   \n",
        "  \n",
        "  \n",
        "3. $\\hat{x_i}$ = $\\frac {x_i - \\mu_B} {\\sqrt{\\sigma_B^2 + \\epsilon}}$\n",
        "  \n",
        "    \n",
        "4. $y_i$ = $\\gamma\\hat{x_i}$ + $\\beta$\n",
        "\n",
        "## 4. 특징\n",
        "- weight initialization이 weight에 대한 processing이라면 batch normalization은 data에 대한 processing이다.    \n",
        "\n",
        "\n",
        "- batch normalization이 학습할 parameter는 2개! $\\gamma, \\beta$ \n",
        " \n",
        "- batch normalization을 쓰면 bias를 포함할 필요가 없음!! 첫 layer부터 이미지를 알아서 평균이랑 분산을 구해서 정규화시켜 넘어감. 이후에는 layer마다 batch normalization 적용해줘야 함!  \n",
        "\n",
        "- 학습 시에는 배치마다 평균과 분산을 구해줘야 한다. \n",
        "하지만 inference 시에는 mini-batch의 값들을 이용하는 대신 지금까지 본 전체 데이터를 다 사용한다는 느낌으로, training 할 때 현재까지 본 input들의 이동평균\n",
        "(moving average) 및 unbiased variance estimate의 이동평균을\n",
        "계산하여 저장해놓은 뒤 이 값으로 normalize를 한다. 마지막에 gamma와 beta를 이용하여 scale/shift 해주는 것은 동일하다.\n",
        "- batch normalization을 쓰면 학습이 잘 되있어 learning rate를 더 크게 줄 수 있다는 장점이 있다.  \n",
        "- batch - convolution - activation 반복 후 fully-connected - activation - dropout 순서로 진행한다. \n",
        "\n",
        "## 5. 장점\n",
        "1. 기존 Deep Network에서는 learning rate를 너무 높게 잡을 경우\n",
        "gradient가 explode/vanish 하거나, 나쁜 local minima에 빠지는 문제가\n",
        "있었다.\n",
        "2. 이는 parameter들의 scale 때문인데, Batch Normalization을 사용할\n",
        "경우 propagation 할 때 parameter의 scale에 영향을 받지 않게 된다.\n",
        "3. 따라서, learning rate를 크게 잡을 수 있게 되고 이는 빠른 학습을\n",
        "가능케 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmP31GT-1G7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0ab846a4-e353-4ee1-871e-238302986fd8"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 271,346\n",
            "Trainable params: 268,978\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31UVS9jrrh2Q",
        "colab_type": "text"
      },
      "source": [
        "Dropout\n",
        "====\n",
        "- overfitting을 방지할 수 있는 방법  \n",
        "- 일반적으로 마지막 3개의 layer에만 dropout 적용하며 출력 layer에는 dropout 적용하지 않음\n",
        "\n",
        "### Mote Carlo Dropout\n",
        "반복 훈련 후 나오는 값의 평균을 예측값으로 선택하는 방식으로 test의 성능을 높힐 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQE4N9gBsKVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "4bfe06d3-2c7d-42e7-951c-1c6d284e50f2"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28,28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71eCyrCjuaAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}