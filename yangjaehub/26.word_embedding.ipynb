{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_dict : {'을': 1, '나': 2, '는': 3, '치킨': 4, '오늘': 5, '저녁': 6, '에': 7, '먹': 8, '예정': 9, '입니다': 10, '어제': 11, '맥주': 12, '와': 13, '함께': 14, '먹었': 15, '습니다': 16}\n",
      "\n",
      "Token to sequences\n",
      " [[2], [3], [5], [6], [7], [4], [1], [8], [1], [9], [10]] \n",
      " [[2], [3], [11], [12], [13], [14], [4], [1], [15], [16]]\n",
      "\n",
      "Token as list\n",
      " [2, 3, 5, 6, 7, 4, 1, 8, 1, 9, 10] \n",
      " [2, 3, 11, 12, 13, 14, 4, 1, 15, 16] \n",
      "\n",
      "원-핫 인코딩된 문장1: [0. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      "원-핫 인코딩된 문장2: [0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.]\n",
      "두 문장의 코사인 유사도: 0.46225014\n",
      "원-핫 인코딩의 길이가 5,000일 때의 코사인 유사도: 0.438529\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "sentence1 = ['나','는','오늘','저녁','에','치킨','을','먹','을','예정','입니다']\n",
    "sentence2 = ['나','는','어제', '맥주','와', '함께', '치킨','을', '먹었', '습니다']\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokenizer=Tokenizer()\n",
    "\n",
    "    tokenizer.fit_on_texts(sentence1 + sentence2)\n",
    "    word_dict = tokenizer.word_index\n",
    "    print('word_dict :', word_dict)\n",
    "\n",
    "    sen1 = tokenizer.texts_to_sequences(sentence1)\n",
    "    sen2 = tokenizer.texts_to_sequences(sentence2)\n",
    "    print('\\nToken to sequences\\n', sen1, '\\n', sen2)\n",
    "\n",
    "    sen1 = [ token[0] for token in sen1]\n",
    "    sen2 = [ token[0] for token in sen2]\n",
    "    print('\\nToken as list\\n', sen1, '\\n', sen2, '\\n')\n",
    "    \n",
    "    oh_sen1 = sum(tf.one_hot(sen1, len(word_dict)))\n",
    "    oh_sen2 = sum(tf.one_hot(sen2, len(word_dict)))\n",
    "\n",
    "    print(\"원-핫 인코딩된 문장1:\", oh_sen1.numpy())\n",
    "    print(\"원-핫 인코딩된 문장2:\", oh_sen2.numpy())\n",
    "\n",
    "    \n",
    "    cos_simil = (sum(list(oh_sen1 * oh_sen2)) / (tf.norm(oh_sen1) * tf.norm(oh_sen2))).numpy()\n",
    "    print(\"두 문장의 코사인 유사도:\", cos_simil)\n",
    "\n",
    "    \n",
    "    len_word=5000\n",
    "\n",
    "    oh_sen1 = sum(tf.one_hot(sen1, len_word))\n",
    "    oh_sen2 = sum(tf.one_hot(sen2, len_word))\n",
    "    cos_simil = (sum(list(oh_sen1 * oh_sen2)) / (tf.norm(oh_sen1) * tf.norm(oh_sen2))).numpy()\n",
    "\n",
    "    print(\"원-핫 인코딩의 길이가 5,000일 때의 코사인 유사도:\", cos_simil)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
